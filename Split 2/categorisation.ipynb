{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "import mysql\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from mysql.connector import Error\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = defaultdict(TrieNode)\n",
    "        self.is_end = False\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, conversation):\n",
    "        node = self.root\n",
    "        for tweet_id in conversation:\n",
    "            node = node.children[tweet_id]\n",
    "        node.is_end = True\n",
    "\n",
    "    def is_subset(self, conversation):\n",
    "        node = self.root\n",
    "        for tweet_id in conversation:\n",
    "            if tweet_id not in node.children:\n",
    "                return False\n",
    "            node = node.children[tweet_id]\n",
    "        return True\n",
    "\n",
    "def trace_conversation(start_tweet_id: str, tweet_dict: dict):\n",
    "    convo = []\n",
    "    current_tweet_id = start_tweet_id\n",
    "    users_in_conversation = set()\n",
    "    local_processed_tweet_ids = set()  # Local set to track the current conversation\n",
    "    while current_tweet_id:\n",
    "        if current_tweet_id not in tweet_dict or current_tweet_id in local_processed_tweet_ids:\n",
    "            break\n",
    "        tweet_info = tweet_dict[current_tweet_id]\n",
    "        convo.append(current_tweet_id)\n",
    "        users_in_conversation.add(tweet_info['user_id'])\n",
    "        local_processed_tweet_ids.add(current_tweet_id)\n",
    "        if len(users_in_conversation) > 2:\n",
    "            return convo[:-1][::-1]  # As soon as the third user appears, we delete his tweet and return\n",
    "        current_tweet_id = tweet_info['replied_tweet_id']\n",
    "    return convo[::-1] if len(users_in_conversation) == 2 else None\n",
    "\n",
    "def extract_and_filter_conversations(df: pd.DataFrame):\n",
    "    df = df.sort_values(\"tweet_creation_time\", ascending=False)\n",
    "    df.index = df.index.astype(str)\n",
    "    tweet_dict = df.to_dict('index')\n",
    "    conversations = []\n",
    "    trie = Trie()  # Initialize trie for subset checks\n",
    "\n",
    "    # Start tracing conversations from tweets that are replies\n",
    "    for tweet_id in tqdm(df[df['replied_tweet_id'].notnull()].index, desc=\"Extracting all conversations\"):\n",
    "        if conversation := trace_conversation(tweet_id, tweet_dict):\n",
    "            if not trie.is_subset(conversation):\n",
    "                trie.insert(conversation)\n",
    "                conversations.append(conversation)\n",
    "\n",
    "    return conversations\n",
    "\n",
    "\n",
    "def get_local_data(query: str, path: str, dtype: bool = True) -> pd.DataFrame:\n",
    "    # Connect to the SQLite database using a context manager\n",
    "    with sqlite3.connect(path) as connection:\n",
    "        # Read the data into a DataFrame\n",
    "        if dtype:\n",
    "            df = pd.read_sql_query(query, connection,\n",
    "                                   dtype=DTYPES,\n",
    "                                   index_col='tweet_id')\n",
    "            df['tweet_creation_time'] = pd.to_datetime(df['tweet_creation_time'])\n",
    "            df['user_creation_time'] = pd.to_datetime(df['user_creation_time'])\n",
    "        else:\n",
    "            df = pd.read_sql_query(query, connection)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# def fetch_data(query: str, dtype: bool = True) -> pd.DataFrame:\n",
    "#     engine = create_engine(f\"mysql://{USER}:{PASSWORD}@{HOST}:3306/{DATABASE}\")\n",
    "#     if dtype:\n",
    "#         return pd.read_sql_query(query, engine,\n",
    "#                                  dtype=DTYPES, index_col='tweet_id')\n",
    "#     return pd.read_sql_query(query, engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_given_var(env_var_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if the given environment variable is set and return its value.\n",
    "\n",
    "    Args:\n",
    "        env_var_str (str): The name of the environment variable to check.\n",
    "\n",
    "    Returns:\n",
    "        str: The value of the environment variable.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the environment variable is not found.\n",
    "    \"\"\"\n",
    "\n",
    "    env_var = os.getenv(env_var_str)\n",
    "    assert (\n",
    "        env_var is not None\n",
    "    ), f\"{env_var_str} is required but not found in environment variables\"\n",
    "    return env_var\n",
    "\n",
    "\n",
    "def check_env_vars() -> (str, str, str, str):  # type: ignore\n",
    "    user = check_given_var(\"DBL_USER\")\n",
    "    database = check_given_var(\"DBL_DATABASE\")\n",
    "    password = check_given_var(\"DBL_PASSWORD\")\n",
    "    host = check_given_var(\"DBL_HOST\")\n",
    "    return user, database, password, host\n",
    "\n",
    "\n",
    "USER, DATABASE, PASSWORD, HOST = check_env_vars()\n",
    "# USER, DATABASE = \"nezox2um_test\", \"nezox2um_test\"\n",
    "QUERY_ALL = \"\"\"\n",
    "SELECT \n",
    "    Users.user_id AS user_id, \n",
    "    Users.creation_time AS user_creation_time, \n",
    "    Tweets.creation_time AS tweet_creation_time,\n",
    "    Tweets.tweet_id,\n",
    "    Tweets.full_text,\n",
    "    Tweets.lang,\n",
    "    Tweets.replied_tweet_id\n",
    "FROM Users\n",
    "INNER JOIN Tweets ON Users.user_id = Tweets.user_id;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "DTYPES = {\n",
    "\"user_id\": \"object\",\n",
    "\"tweet_id\": \"object\",\n",
    "\"full_text\": \"object\",\n",
    "\"lang\": \"category\",\n",
    "\"replied_tweet_id\": \"object\",\n",
    "}\n",
    "\n",
    "COMPANY_NAME_TO_ID = {\n",
    "    \"Klm\": \"56377143\",\n",
    "    \"Air France\": \"106062176\",\n",
    "    \"British Airways\": \"18332190\",\n",
    "    \"American Air\": \"22536055\",\n",
    "    \"Lufthansa\": \"124476322\",\n",
    "    \"Air Berlin\": \"26223583\",\n",
    "    \"Air Berlin assist\": \"2182373406\",\n",
    "    \"easyJet\": \"38676903\",\n",
    "    \"Ryanair\": \"1542862735\",\n",
    "    \"Singapore Airlines\": \"253340062\",\n",
    "    \"Qantas\": \"218730857\",\n",
    "    \"Etihad Airways\": \"45621423\",\n",
    "    \"Virgin Atlantic\": \"20626359\",\n",
    "}\n",
    "\n",
    "COMPANY_ID_TO_NAME = {v: k for k, v in COMPANY_NAME_TO_ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server\n",
    "# test_data = fetch_data(QUERY_ALL)\n",
    "# Local\n",
    "path =  os.path.join(\n",
    "        os.path.dirname(\n",
    "            os.getcwd()\n",
    "        ),\n",
    "    \"data_processed\", \"local_backup.db\")\n",
    "\n",
    "test_data = get_local_data(QUERY_ALL, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>replied_tweet_id</th>\n",
       "      <th>tweet_creation_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1131172858951024641</th>\n",
       "      <td>393374091</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-05-22 12:20:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130922003702177800</th>\n",
       "      <td>880417607865815040</td>\n",
       "      <td>1130615560910254080</td>\n",
       "      <td>2019-05-21 19:43:11+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131172864147808257</th>\n",
       "      <td>3420691215</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-05-22 12:20:01+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131172867985485824</th>\n",
       "      <td>394376606</td>\n",
       "      <td>1131032916232826881</td>\n",
       "      <td>2019-05-22 12:20:02+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131030279278063616</th>\n",
       "      <td>227687574</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-05-22 02:53:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696703690772485</th>\n",
       "      <td>278698748</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-03-30 18:43:14+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696708983984131</th>\n",
       "      <td>246520593</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-03-30 18:43:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696710447800320</th>\n",
       "      <td>109284383</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-03-30 18:43:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696713350217728</th>\n",
       "      <td>1223576386432126976</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-03-30 18:43:16+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696713765564416</th>\n",
       "      <td>56784613</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-03-30 18:43:16+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6148105 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 user_id     replied_tweet_id  \\\n",
       "tweet_id                                                        \n",
       "1131172858951024641            393374091                 None   \n",
       "1130922003702177800   880417607865815040  1130615560910254080   \n",
       "1131172864147808257           3420691215                 None   \n",
       "1131172867985485824            394376606  1131032916232826881   \n",
       "1131030279278063616            227687574                 None   \n",
       "...                                  ...                  ...   \n",
       "1244696703690772485            278698748                 None   \n",
       "1244696708983984131            246520593                 None   \n",
       "1244696710447800320            109284383                 None   \n",
       "1244696713350217728  1223576386432126976                 None   \n",
       "1244696713765564416             56784613                 None   \n",
       "\n",
       "                          tweet_creation_time  \n",
       "tweet_id                                       \n",
       "1131172858951024641 2019-05-22 12:20:00+00:00  \n",
       "1130922003702177800 2019-05-21 19:43:11+00:00  \n",
       "1131172864147808257 2019-05-22 12:20:01+00:00  \n",
       "1131172867985485824 2019-05-22 12:20:02+00:00  \n",
       "1131030279278063616 2019-05-22 02:53:26+00:00  \n",
       "...                                       ...  \n",
       "1244696703690772485 2020-03-30 18:43:14+00:00  \n",
       "1244696708983984131 2020-03-30 18:43:15+00:00  \n",
       "1244696710447800320 2020-03-30 18:43:15+00:00  \n",
       "1244696713350217728 2020-03-30 18:43:16+00:00  \n",
       "1244696713765564416 2020-03-30 18:43:16+00:00  \n",
       "\n",
       "[6148105 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_special = test_data[[\"user_id\", \"replied_tweet_id\", \"tweet_creation_time\"]]\n",
    "convo_special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting all conversations: 100%|██████████| 1795409/1795409 [00:08<00:00, 213357.09it/s]\n"
     ]
    }
   ],
   "source": [
    "conversations = extract_and_filter_conversations(convo_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conversation</th>\n",
       "      <th>Tweet_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1244694453190897664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1244696682979303426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1244677304598609923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1244696641401163776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1244648694454026240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712242</th>\n",
       "      <td>1064150</td>\n",
       "      <td>451125255294443521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712243</th>\n",
       "      <td>1064151</td>\n",
       "      <td>430790355962052608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712244</th>\n",
       "      <td>1064151</td>\n",
       "      <td>430792524043931648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712245</th>\n",
       "      <td>1064152</td>\n",
       "      <td>248528541157834752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712246</th>\n",
       "      <td>1064152</td>\n",
       "      <td>248529937198366720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2712247 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Conversation             Tweet_ID\n",
       "0                   1  1244694453190897664\n",
       "1                   1  1244696682979303426\n",
       "2                   2  1244677304598609923\n",
       "3                   2  1244696641401163776\n",
       "4                   3  1244648694454026240\n",
       "...               ...                  ...\n",
       "2712242       1064150   451125255294443521\n",
       "2712243       1064151   430790355962052608\n",
       "2712244       1064151   430792524043931648\n",
       "2712245       1064152   248528541157834752\n",
       "2712246       1064152   248529937198366720\n",
       "\n",
       "[2712247 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for convo_num, convo in enumerate(conversations, start=1):\n",
    "    data.extend((convo_num, tweet_id) for tweet_id in convo)\n",
    "# Create a DataFrame\n",
    "df_conversations = pd.DataFrame(data, columns=['Conversation', 'Tweet_ID'])\n",
    "\n",
    "# Set MultiIndex\n",
    "df_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_creation_time</th>\n",
       "      <th>tweet_creation_time</th>\n",
       "      <th>full_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>replied_tweet_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conversation</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1244694453190897664</th>\n",
       "      <td>521835883</td>\n",
       "      <td>2012-03-12 01:11:22+00:00</td>\n",
       "      <td>2020-03-30 18:34:17+00:00</td>\n",
       "      <td>@nealrach @VirginAtlantic Siiiigh.... Still no...</td>\n",
       "      <td>en</td>\n",
       "      <td>1243885949697888263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696682979303426</th>\n",
       "      <td>20626359</td>\n",
       "      <td>2009-02-11 20:50:56+00:00</td>\n",
       "      <td>2020-03-30 18:43:09+00:00</td>\n",
       "      <td>@Jade_Velveteese Hi Jade. We have an ‘Away fro...</td>\n",
       "      <td>en</td>\n",
       "      <td>1244694453190897664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>1244677304598609923</th>\n",
       "      <td>396021583</td>\n",
       "      <td>2011-10-22 16:35:05+00:00</td>\n",
       "      <td>2020-03-30 17:26:09+00:00</td>\n",
       "      <td>@VirginAtlantic Sod off your primary sharehold...</td>\n",
       "      <td>en</td>\n",
       "      <td>1244669964289806338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696641401163776</th>\n",
       "      <td>832964639436701696</td>\n",
       "      <td>2017-02-18 14:47:00+00:00</td>\n",
       "      <td>2020-03-30 18:42:59+00:00</td>\n",
       "      <td>@Boyde11 @VirginAtlantic Get your facts right,...</td>\n",
       "      <td>en</td>\n",
       "      <td>1244677304598609923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>1244648694454026240</th>\n",
       "      <td>1233410199500791809</td>\n",
       "      <td>2020-02-28 15:14:56+00:00</td>\n",
       "      <td>2020-03-30 15:32:27+00:00</td>\n",
       "      <td>@flavioArCab @Chapux0204 @chechiffss @aeronaut...</td>\n",
       "      <td>es</td>\n",
       "      <td>1244643427515535360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064150</th>\n",
       "      <th>451125255294443521</th>\n",
       "      <td>22536055</td>\n",
       "      <td>2009-03-02 21:23:05+00:00</td>\n",
       "      <td>2014-04-01 22:33:37+00:00</td>\n",
       "      <td>@lanaupdates_ Your information has been forwar...</td>\n",
       "      <td>en</td>\n",
       "      <td>451124070730719233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1064151</th>\n",
       "      <th>430790355962052608</th>\n",
       "      <td>64327804</td>\n",
       "      <td>2009-08-10 03:34:27+00:00</td>\n",
       "      <td>2014-02-04 19:49:59+00:00</td>\n",
       "      <td>@AmericanAir phew, they finally turned on the ...</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430792524043931648</th>\n",
       "      <td>22536055</td>\n",
       "      <td>2009-03-02 21:23:05+00:00</td>\n",
       "      <td>2014-02-04 19:58:36+00:00</td>\n",
       "      <td>@benjy_greenberg It looks like we'll have you ...</td>\n",
       "      <td>en</td>\n",
       "      <td>430790355962052608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1064152</th>\n",
       "      <th>248528541157834752</th>\n",
       "      <td>19911051</td>\n",
       "      <td>2009-02-02 15:17:02+00:00</td>\n",
       "      <td>2012-09-19 21:06:36+00:00</td>\n",
       "      <td>Un-fucking believable!\\nThanks @BritishAirways...</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248529937198366720</th>\n",
       "      <td>399494759</td>\n",
       "      <td>2011-10-27 15:47:03+00:00</td>\n",
       "      <td>2012-09-19 21:12:09+00:00</td>\n",
       "      <td>@djmarkknight @britishairways i have have the ...</td>\n",
       "      <td>en</td>\n",
       "      <td>248528541157834752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2712247 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              user_id  \\\n",
       "Conversation Tweet_ID                                   \n",
       "1            1244694453190897664            521835883   \n",
       "             1244696682979303426             20626359   \n",
       "2            1244677304598609923            396021583   \n",
       "             1244696641401163776   832964639436701696   \n",
       "3            1244648694454026240  1233410199500791809   \n",
       "...                                               ...   \n",
       "1064150      451125255294443521              22536055   \n",
       "1064151      430790355962052608              64327804   \n",
       "             430792524043931648              22536055   \n",
       "1064152      248528541157834752              19911051   \n",
       "             248529937198366720             399494759   \n",
       "\n",
       "                                        user_creation_time  \\\n",
       "Conversation Tweet_ID                                        \n",
       "1            1244694453190897664 2012-03-12 01:11:22+00:00   \n",
       "             1244696682979303426 2009-02-11 20:50:56+00:00   \n",
       "2            1244677304598609923 2011-10-22 16:35:05+00:00   \n",
       "             1244696641401163776 2017-02-18 14:47:00+00:00   \n",
       "3            1244648694454026240 2020-02-28 15:14:56+00:00   \n",
       "...                                                    ...   \n",
       "1064150      451125255294443521  2009-03-02 21:23:05+00:00   \n",
       "1064151      430790355962052608  2009-08-10 03:34:27+00:00   \n",
       "             430792524043931648  2009-03-02 21:23:05+00:00   \n",
       "1064152      248528541157834752  2009-02-02 15:17:02+00:00   \n",
       "             248529937198366720  2011-10-27 15:47:03+00:00   \n",
       "\n",
       "                                       tweet_creation_time  \\\n",
       "Conversation Tweet_ID                                        \n",
       "1            1244694453190897664 2020-03-30 18:34:17+00:00   \n",
       "             1244696682979303426 2020-03-30 18:43:09+00:00   \n",
       "2            1244677304598609923 2020-03-30 17:26:09+00:00   \n",
       "             1244696641401163776 2020-03-30 18:42:59+00:00   \n",
       "3            1244648694454026240 2020-03-30 15:32:27+00:00   \n",
       "...                                                    ...   \n",
       "1064150      451125255294443521  2014-04-01 22:33:37+00:00   \n",
       "1064151      430790355962052608  2014-02-04 19:49:59+00:00   \n",
       "             430792524043931648  2014-02-04 19:58:36+00:00   \n",
       "1064152      248528541157834752  2012-09-19 21:06:36+00:00   \n",
       "             248529937198366720  2012-09-19 21:12:09+00:00   \n",
       "\n",
       "                                                                          full_text  \\\n",
       "Conversation Tweet_ID                                                                 \n",
       "1            1244694453190897664  @nealrach @VirginAtlantic Siiiigh.... Still no...   \n",
       "             1244696682979303426  @Jade_Velveteese Hi Jade. We have an ‘Away fro...   \n",
       "2            1244677304598609923  @VirginAtlantic Sod off your primary sharehold...   \n",
       "             1244696641401163776  @Boyde11 @VirginAtlantic Get your facts right,...   \n",
       "3            1244648694454026240  @flavioArCab @Chapux0204 @chechiffss @aeronaut...   \n",
       "...                                                                             ...   \n",
       "1064150      451125255294443521   @lanaupdates_ Your information has been forwar...   \n",
       "1064151      430790355962052608   @AmericanAir phew, they finally turned on the ...   \n",
       "             430792524043931648   @benjy_greenberg It looks like we'll have you ...   \n",
       "1064152      248528541157834752   Un-fucking believable!\\nThanks @BritishAirways...   \n",
       "             248529937198366720   @djmarkknight @britishairways i have have the ...   \n",
       "\n",
       "                                 lang     replied_tweet_id  \n",
       "Conversation Tweet_ID                                       \n",
       "1            1244694453190897664   en  1243885949697888263  \n",
       "             1244696682979303426   en  1244694453190897664  \n",
       "2            1244677304598609923   en  1244669964289806338  \n",
       "             1244696641401163776   en  1244677304598609923  \n",
       "3            1244648694454026240   es  1244643427515535360  \n",
       "...                               ...                  ...  \n",
       "1064150      451125255294443521    en   451124070730719233  \n",
       "1064151      430790355962052608    en                 None  \n",
       "             430792524043931648    en   430790355962052608  \n",
       "1064152      248528541157834752    en                 None  \n",
       "             248529937198366720    en   248528541157834752  \n",
       "\n",
       "[2712247 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Merge the conversation DataFrame with the test_data DataFrame\n",
    "df_conversations_full = df_conversations.merge(test_data, left_on='Tweet_ID', right_index=True, how='left')\n",
    "\n",
    "# Set the MultiIndex again with Conversation and Tweet_ID\n",
    "df_conversations_full.set_index(['Conversation', 'Tweet_ID'], inplace=True)\n",
    "df_conversations_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chekm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Chekm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flights': 172511, 'contact': 30650, 'countries': 4055, 'entry': 1882, 'communication': 6896, 'refund': 70008, 'vouchers': 7331, 'flight': 558262, 'change': 30110, 'fees': 10413, 'problems': 8868, 'response': 35817, 'booking': 46647, 'service': 101579, 'problem': 25005, 'status': 21981, 'book': 35219, 'website': 28097, 'inquiries': 303, 'refunds': 9735, 'inflight': 1003, 'crew': 41119, 'staff': 48342, 'voucher': 42696, 'ticket': 47564, 'payment': 5215, 'cancellation': 11223, 'baggage': 39276, 'seats': 41640, 'policy': 20969, 'representative': 1405, 'health': 5452, 'online': 18001, 'silver': 1750, 'membership': 2172, 'tier': 1140, 'review': 3202, 'assistance': 20342, 'agent': 17137, 'check': 17643, 'help': 39104, 'courtesy': 1968, 'magazine': 1029, 'cost': 10536, 'quarantine': 1178, 'mobile': 661, 'emergency': 7302, 'delay': 47703, 'tickets': 20540, 'complaints': 3637, 'error': 8048, 'changes': 12502, 'music': 2714, 'id': 3999, 'restrictions': 8215, 'claim': 28981, 'reschedule': 631, 'issues': 18346, 'safety': 11669, 'points': 8784, 'support': 13279, 'cancel': 3280, 'luggage': 41950, 'issue': 27847, 'internet': 1960, 'gate': 30271, 'app': 19525, 'food': 17234, 'attitude': 2645, 'upgrades': 2391, 'upgrade': 6476, 'terminal': 7273, 'complaint': 13442, 'insurance': 5833, 'seat': 69341, 'visa': 2473, 'security': 11687, 'passport': 8159, 'immigration': 1082, 'miles': 17901, 'maintenance': 6666, 'meal': 6650, 'connection': 14625, 'gold': 4860, 'loyalty': 3807, 'comment': 2998, 'compensation': 35085, 'space': 9852, 'experience': 32081, 'itinerary': 1515, 'apology': 5318, 'dirty': 750, 'border': 1205, 'entertainment': 4399, 'rude': 2926, 'appreciated': 47, 'boarding': 9734, 'legroom': 2361, 'storm': 3276, 'platinum': 5530, 'connections': 2468, 'toilet': 1699, 'bug': 441, 'webpage': 402, 'fix': 1624, 'suggestion': 1154, 'charging': 184, 'cold': 479, 'feedback': 17931, 'damage': 2180, 'comfort': 3120, 'delays': 16790, 'device': 1828, 'program': 1877, 'drinks': 2592, 'hot': 629, 'lost': 3628, 'newspaper': 334, 'found': 1523, 'blanket': 4550, 'evacuation': 416, 'exit': 3101, 'weather': 12990, 'survey': 801, 'inquiry': 314, 'password': 1305, 'behavior': 7371, 'seating': 1736, 'offline': 66, 'rating': 1381, 'customs': 1174, 'ventilation': 41, 'outlets': 1303, 'snack': 1019, 'item': 3035, 'delayed': 5085, 'failure': 2342, 'water': 6523, 'mileage': 1043, 'items': 5459, 'missing': 311, 'wifi': 4713, 'tea': 1260, 'elite': 643, 'glitch': 583, 'cancelations': 192, 'coffee': 2584, 'clean': 314, 'turbulence': 762, 'restriction': 763, 'allergy': 823, 'lavatory': 174, 'alcohol': 808, 'temperature': 495, 'sockets': 126, 'beverage': 966, 'amenity': 407, 'recommendation': 303, 'compensate': 352, 'tight': 191, 'movie': 1644, 'outlet': 262, 'hygiene': 214, 'juice': 283, 'malfunction': 238, 'cleanliness': 228, 'stolen': 79, 'sanitation': 41, 'assignment': 467, 'bathroom': 965, 'amenities': 338, 'smell': 342, 'vegan': 476, 'diabetic': 39, 'broken': 733, 'headphones': 2617, 'politeness': 89, 'pillow': 235, 'movies': 857, 'soap': 130, 'overbooked': 29, 'vegetarian': 98, 'overbooking': 118, 'towel': 154, 'username': 40, 'napkin': 27, 'recline': 1924, 'socket': 107, 'damaged': 70, 'cramped': 77, 'restroom': 335, 'halal': 100, 'earphones': 99, 'kosher': 107, 'missed': 37, 'unfriendly': 11, 'uncomfortable': 26, 'assign': 40, 'odor': 55}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "noun_counts = {}\n",
    "chosen_categories = [\n",
    "    # General Travel and Flight Related Issues\n",
    "    'flight', 'flights', 'delay', 'delays', 'cancellation', 'cancelations', 'cancel', 'missed', 'connection', \n",
    "    'connections', 'boarding', 'gate', 'terminal', 'security', 'check', 'check-in', 'ticket', 'tickets', 'booking', \n",
    "    'book', 'itinerary', 'reschedule', 'upgrade', 'upgrades', 'overbooked', 'overbooking', 'seat', 'seats', 'assignment', \n",
    "    'assign', 'seating', 'boarding pass', 'baggage', 'luggage', 'lost luggage', 'damaged luggage', 'carry-on', 'checked bag', \n",
    "    'baggage claim', 'baggage policy', 'fees', 'cost', 'extra cost', 'payment', 'payment issue', 'refund', 'refunds', \n",
    "    'compensation', 'voucher', 'vouchers', 'policy', 'change', 'changes', 'insurance', 'restriction', 'restrictions', \n",
    "    'travel document', 'passport', 'visa', 'id', 'driver license', 'entry', 'exit', 'countries', 'border', 'immigration', \n",
    "    'customs', 'quarantine', 'health', 'safety', 'emergency', 'evacuation', 'weather', 'storm', 'turbulence', 'mechanical issue', \n",
    "    'maintenance', 'technical issue', 'failure', 'glitch', 'malfunction', 'wifi', 'internet', 'entertainment', 'inflight', \n",
    "    'movie', 'movies', 'music', 'magazine', 'newspaper', 'meal', 'snack', 'beverage', 'drinks', 'water', 'juice', 'alcohol', \n",
    "    'coffee', 'tea', 'soft drink', 'food', 'meal', 'special meal', 'vegetarian', 'vegan', 'kosher', 'halal', 'allergy', \n",
    "    'child meal', 'gluten-free', 'lactose-free', 'diabetic', 'service', 'crew', 'staff', 'attitude', 'behavior', 'courtesy', \n",
    "    'politeness', 'rude', 'unfriendly', 'help', 'assistance', 'communication', 'response', 'contact', 'call center', 'support', \n",
    "    'help desk', 'customer service', 'agent', 'representative', 'complaint', 'complaints', 'issue', 'issues', 'problem', 'problems', \n",
    "    'inquiry', 'inquiries', 'feedback', 'review', 'survey', 'rating', 'recommendation', 'suggestion', 'comment', 'experience', \n",
    "    'appreciated', 'apology', 'compensate', 'voucher', 'vouchers', 'upgrade', 'upgrades', 'mileage', 'miles', 'points', 'membership', \n",
    "    'tier', 'status', 'gold', 'silver', 'platinum', 'elite', 'frequent flyer', 'loyalty', 'program', 'amenity', 'amenities', 'seating', \n",
    "    'legroom', 'space', 'recline', 'comfort', 'uncomfortable', 'tight', 'cramped', 'cold', 'hot', 'temperature', 'ventilation', \n",
    "    'cleanliness', 'hygiene', 'sanitation', 'dirty', 'clean', 'restroom', 'bathroom', 'toilet', 'lavatory', 'smell', 'odor', \n",
    "    'hygiene product', 'soap', 'towel', 'napkin', 'blanket', 'pillow', 'headphones', 'earphones', 'charging', 'outlet', 'outlets', \n",
    "    'socket', 'sockets', 'lost and found', 'item', 'items', 'claim', 'lost', 'found', 'delayed', 'missing', 'stolen', 'damage', \n",
    "    'damaged', 'broken', 'compensation', 'refund', 'policy', 'restrictions', 'website', 'webpage', 'online', 'offline', 'app', \n",
    "    'mobile', 'device', 'error', 'bug', 'fix', 'technical issue', 'log in', 'sign in', 'password', 'username', 'error', 'malfunction'\n",
    "]\n",
    "\n",
    "def ReturnNouns(sentence: str):\n",
    "    #nouns\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    tagged = pos_tag(tokens)\n",
    "    return [word for word, pos in tagged if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "\n",
    "# Assuming df_conversations_full is your DataFrame\n",
    "df_test = df_conversations_full\n",
    "for index, row in df_test.iterrows():\n",
    "    if row['lang'] == 'en':\n",
    "        sentence = row['full_text']\n",
    "        nouns = ReturnNouns(sentence)\n",
    "        \n",
    "        # Tokenize the sentence to get the words\n",
    "        words_in_sentence = word_tokenize(sentence)\n",
    "        \n",
    "        # Check for overlapping elements\n",
    "        overlap_exists = any(element in words_in_sentence for element in chosen_categories)\n",
    "        \n",
    "        for noun in nouns:\n",
    "            if noun.lower() in chosen_categories:  # Only increment if noun is in chosen categories\n",
    "                noun_counts.setdefault(noun.lower(), 0)\n",
    "                noun_counts[noun.lower()] += 1\n",
    "\n",
    "print(noun_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flights': 172511,\n",
       " 'contact': 30650,\n",
       " 'countries': 4055,\n",
       " 'entry': 1882,\n",
       " 'communication': 6896,\n",
       " 'refund': 70008,\n",
       " 'vouchers': 7331,\n",
       " 'flight': 558262,\n",
       " 'change': 30110,\n",
       " 'fees': 10413,\n",
       " 'problems': 8868,\n",
       " 'response': 35817,\n",
       " 'booking': 46647,\n",
       " 'service': 101579,\n",
       " 'problem': 25005,\n",
       " 'status': 21981,\n",
       " 'book': 35219,\n",
       " 'website': 28097,\n",
       " 'inquiries': 303,\n",
       " 'refunds': 9735,\n",
       " 'inflight': 1003,\n",
       " 'crew': 41119,\n",
       " 'staff': 48342,\n",
       " 'voucher': 42696,\n",
       " 'ticket': 47564,\n",
       " 'payment': 5215,\n",
       " 'cancellation': 11223,\n",
       " 'baggage': 39276,\n",
       " 'seats': 41640,\n",
       " 'policy': 20969,\n",
       " 'representative': 1405,\n",
       " 'health': 5452,\n",
       " 'online': 18001,\n",
       " 'silver': 1750,\n",
       " 'membership': 2172,\n",
       " 'tier': 1140,\n",
       " 'review': 3202,\n",
       " 'assistance': 20342,\n",
       " 'agent': 17137,\n",
       " 'check': 17643,\n",
       " 'help': 39104,\n",
       " 'courtesy': 1968,\n",
       " 'magazine': 1029,\n",
       " 'cost': 10536,\n",
       " 'quarantine': 1178,\n",
       " 'mobile': 661,\n",
       " 'emergency': 7302,\n",
       " 'delay': 47703,\n",
       " 'tickets': 20540,\n",
       " 'complaints': 3637,\n",
       " 'error': 8048,\n",
       " 'changes': 12502,\n",
       " 'music': 2714,\n",
       " 'id': 3999,\n",
       " 'restrictions': 8215,\n",
       " 'claim': 28981,\n",
       " 'reschedule': 631,\n",
       " 'issues': 18346,\n",
       " 'safety': 11669,\n",
       " 'points': 8784,\n",
       " 'support': 13279,\n",
       " 'cancel': 3280,\n",
       " 'luggage': 41950,\n",
       " 'issue': 27847,\n",
       " 'internet': 1960,\n",
       " 'gate': 30271,\n",
       " 'app': 19525,\n",
       " 'food': 17234,\n",
       " 'attitude': 2645,\n",
       " 'upgrades': 2391,\n",
       " 'upgrade': 6476,\n",
       " 'terminal': 7273,\n",
       " 'complaint': 13442,\n",
       " 'insurance': 5833,\n",
       " 'seat': 69341,\n",
       " 'visa': 2473,\n",
       " 'security': 11687,\n",
       " 'passport': 8159,\n",
       " 'immigration': 1082,\n",
       " 'miles': 17901,\n",
       " 'maintenance': 6666,\n",
       " 'meal': 6650,\n",
       " 'connection': 14625,\n",
       " 'gold': 4860,\n",
       " 'loyalty': 3807,\n",
       " 'comment': 2998,\n",
       " 'compensation': 35085,\n",
       " 'space': 9852,\n",
       " 'experience': 32081,\n",
       " 'itinerary': 1515,\n",
       " 'apology': 5318,\n",
       " 'dirty': 750,\n",
       " 'border': 1205,\n",
       " 'entertainment': 4399,\n",
       " 'rude': 2926,\n",
       " 'appreciated': 47,\n",
       " 'boarding': 9734,\n",
       " 'legroom': 2361,\n",
       " 'storm': 3276,\n",
       " 'platinum': 5530,\n",
       " 'connections': 2468,\n",
       " 'toilet': 1699,\n",
       " 'bug': 441,\n",
       " 'webpage': 402,\n",
       " 'fix': 1624,\n",
       " 'suggestion': 1154,\n",
       " 'charging': 184,\n",
       " 'cold': 479,\n",
       " 'feedback': 17931,\n",
       " 'damage': 2180,\n",
       " 'comfort': 3120,\n",
       " 'delays': 16790,\n",
       " 'device': 1828,\n",
       " 'program': 1877,\n",
       " 'drinks': 2592,\n",
       " 'hot': 629,\n",
       " 'lost': 3628,\n",
       " 'newspaper': 334,\n",
       " 'found': 1523,\n",
       " 'blanket': 4550,\n",
       " 'evacuation': 416,\n",
       " 'exit': 3101,\n",
       " 'weather': 12990,\n",
       " 'survey': 801,\n",
       " 'inquiry': 314,\n",
       " 'password': 1305,\n",
       " 'behavior': 7371,\n",
       " 'seating': 1736,\n",
       " 'offline': 66,\n",
       " 'rating': 1381,\n",
       " 'customs': 1174,\n",
       " 'ventilation': 41,\n",
       " 'outlets': 1303,\n",
       " 'snack': 1019,\n",
       " 'item': 3035,\n",
       " 'delayed': 5085,\n",
       " 'failure': 2342,\n",
       " 'water': 6523,\n",
       " 'mileage': 1043,\n",
       " 'items': 5459,\n",
       " 'missing': 311,\n",
       " 'wifi': 4713,\n",
       " 'tea': 1260,\n",
       " 'elite': 643,\n",
       " 'glitch': 583,\n",
       " 'cancelations': 192,\n",
       " 'coffee': 2584,\n",
       " 'clean': 314,\n",
       " 'turbulence': 762,\n",
       " 'restriction': 763,\n",
       " 'allergy': 823,\n",
       " 'lavatory': 174,\n",
       " 'alcohol': 808,\n",
       " 'temperature': 495,\n",
       " 'sockets': 126,\n",
       " 'beverage': 966,\n",
       " 'amenity': 407,\n",
       " 'recommendation': 303,\n",
       " 'compensate': 352,\n",
       " 'tight': 191,\n",
       " 'movie': 1644,\n",
       " 'outlet': 262,\n",
       " 'hygiene': 214,\n",
       " 'juice': 283,\n",
       " 'malfunction': 238,\n",
       " 'cleanliness': 228,\n",
       " 'stolen': 79,\n",
       " 'sanitation': 41,\n",
       " 'assignment': 467,\n",
       " 'bathroom': 965,\n",
       " 'amenities': 338,\n",
       " 'smell': 342,\n",
       " 'vegan': 476,\n",
       " 'diabetic': 39,\n",
       " 'broken': 733,\n",
       " 'headphones': 2617,\n",
       " 'politeness': 89,\n",
       " 'pillow': 235,\n",
       " 'movies': 857,\n",
       " 'soap': 130,\n",
       " 'overbooked': 29,\n",
       " 'vegetarian': 98,\n",
       " 'overbooking': 118,\n",
       " 'towel': 154,\n",
       " 'username': 40,\n",
       " 'napkin': 27,\n",
       " 'recline': 1924,\n",
       " 'socket': 107,\n",
       " 'damaged': 70,\n",
       " 'cramped': 77,\n",
       " 'restroom': 335,\n",
       " 'halal': 100,\n",
       " 'earphones': 99,\n",
       " 'kosher': 107,\n",
       " 'missed': 37,\n",
       " 'unfriendly': 11,\n",
       " 'uncomfortable': 26,\n",
       " 'assign': 40,\n",
       " 'odor': 55}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
