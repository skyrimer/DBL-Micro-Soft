{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import os\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_given_var(env_var_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if the given environment variable is set and return its value.\n",
    "\n",
    "    Args:\n",
    "        env_var_str (str): The name of the environment variable to check.\n",
    "\n",
    "    Returns:\n",
    "        str: The value of the environment variable.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the environment variable is not found.\n",
    "    \"\"\"\n",
    "\n",
    "    env_var = os.getenv(env_var_str)\n",
    "    assert (\n",
    "        env_var is not None\n",
    "    ), f\"{env_var_str} is required but not found in environment variables\"\n",
    "    return env_var\n",
    "\n",
    "\n",
    "def check_env_vars() -> (str, str, str, str):  # type: ignore\n",
    "    user = check_given_var(\"DBL_USER\")\n",
    "    database = check_given_var(\"DBL_DATABASE\")\n",
    "    password = check_given_var(\"DBL_PASSWORD\")\n",
    "    host = check_given_var(\"DBL_HOST\")\n",
    "    return user, database, password, host\n",
    "\n",
    "\n",
    "USER, DATABASE, PASSWORD, HOST = check_env_vars()\n",
    "# USER, DATABASE = \"nezox2um_test\", \"nezox2um_test\"\n",
    "\n",
    "QUERY_ALL = \"\"\"\n",
    "SELECT \n",
    "    Users.user_id AS user_id, \n",
    "    Users.creation_time AS user_creation_time, \n",
    "    Users.verified,\n",
    "    Users.followers_count,\n",
    "    Users.friends_count,\n",
    "    Users.statuses_count,\n",
    "    Users.default_profile,\n",
    "    Users.default_profile_image,\n",
    "    Tweets.creation_time AS tweet_creation_time,\n",
    "    Tweets.tweet_id,\n",
    "    Tweets.full_text,\n",
    "    Tweets.lang,\n",
    "    Tweets.country_code,\n",
    "    Tweets.favorite_count,\n",
    "    Tweets.retweet_count,\n",
    "    Tweets.possibly_sensitive,\n",
    "    Tweets.replied_tweet_id,\n",
    "    Tweets.reply_count,\n",
    "    Tweets.quoted_status_id,\n",
    "    Tweets.quote_count\n",
    "FROM Users\n",
    "INNER JOIN Tweets ON Users.user_id = Tweets.user_id;\n",
    "\"\"\"\n",
    "\n",
    "QUERY_REPLY = \"\"\"\n",
    "SELECT \n",
    "    t1.tweet_id AS tweet_id,\n",
    "    t1.creation_time AS tweet_creation_time,\n",
    "    t1.user_id AS user_id,\n",
    "    t2.tweet_id AS original_tweet_id,\n",
    "    t2.creation_time AS original_tweet_creation_time,\n",
    "    t2.user_id AS original_user_id\n",
    "FROM \n",
    "    Tweets t1\n",
    "INNER JOIN \n",
    "    Tweets t2\n",
    "ON \n",
    "    t1.replied_tweet_id = t2.tweet_id;\n",
    "\"\"\"\n",
    "\n",
    "DTYPES = {\n",
    "\"user_id\": \"object\",\n",
    "\"user_creation_time\": \"datetime64[ns]\",\n",
    "\"verified\": \"bool\",\n",
    "\"followers_count\": \"int32\",\n",
    "\"friends_count\": \"int32\",\n",
    "\"statuses_count\": \"int32\",\n",
    "\"default_profile\": \"bool\",\n",
    "\"default_profile_image\": \"bool\",\n",
    "\"tweet_creation_time\": \"datetime64[ns]\",\n",
    "\"tweet_id\": \"object\",\n",
    "\"full_text\": \"object\",\n",
    "\"lang\": \"category\",\n",
    "\"country_code\": \"category\",\n",
    "\"favorite_count\": \"int32\",\n",
    "\"retweet_count\": \"int32\",\n",
    "\"possibly_sensitive\": \"bool\",\n",
    "\"replied_tweet_id\": \"object\",\n",
    "\"reply_count\": \"int32\",\n",
    "\"quoted_status_id\": \"object\",\n",
    "\"quote_count\": \"int32\",\n",
    "}\n",
    "\n",
    "COMPANY_NAME_TO_ID = {\n",
    "    \"Klm\": \"56377143\",\n",
    "    \"Air France\": \"106062176\",\n",
    "    \"British Airways\": \"18332190\",\n",
    "    \"American Air\": \"22536055\",\n",
    "    \"Lufthansa\": \"124476322\",\n",
    "    \"Air Berlin\": \"26223583\",\n",
    "    \"Air Berlin assist\": \"2182373406\",\n",
    "    \"easyJet\": \"38676903\",\n",
    "    \"Ryanair\": \"1542862735\",\n",
    "    \"Singapore Airlines\": \"253340062\",\n",
    "    \"Qantas\": \"218730857\",\n",
    "    \"Etihad Airways\": \"45621423\",\n",
    "    \"Virgin Atlantic\": \"20626359\",\n",
    "}\n",
    "\n",
    "COMPANY_ID_TO_NAME = {v: k for k, v in COMPANY_NAME_TO_ID.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_to_datetime(blob: str) -> pd.Timestamp:\n",
    "    return pd.to_datetime(blob)\n",
    "\n",
    "\n",
    "\n",
    "def get_local_data(query: str, path: str, dtype: bool = True) -> pd.DataFrame:\n",
    "    # Connect to the SQLite database using a context manager\n",
    "    with sqlite3.connect(path) as connection:\n",
    "        # Read the data into a DataFrame\n",
    "        if dtype:\n",
    "            df = pd.read_sql_query(query, connection,\n",
    "                                   dtype={k: v for k, v in DTYPES.items() if k not in (\"tweet_creation_time\", \"user_creation_time\")},\n",
    "                                   index_col='tweet_id')\n",
    "            df['tweet_creation_time'] = pd.to_datetime(df['tweet_creation_time'])\n",
    "            df['user_creation_time'] = pd.to_datetime(df['user_creation_time'])\n",
    "        else:\n",
    "            df = pd.read_sql_query(query, connection)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_data(query: str, dtype: bool = True) -> pd.DataFrame:\n",
    "    engine = create_engine(f\"mysql://{USER}:{PASSWORD}@{HOST}:3306/{DATABASE}\")\n",
    "    if dtype:\n",
    "        return pd.read_sql_query(query, engine,\n",
    "                                 dtype=DTYPES, index_col='tweet_id')\n",
    "    return pd.read_sql_query(query, engine)\n",
    "\n",
    "\n",
    "def get_full_language_name(language_code: str,\n",
    "                           default: str=\"Undefined Language\") -> str:\n",
    "    \"\"\"\n",
    "    Convert a two-letter language code (ISO 639-1) to its full language name.\n",
    "    \n",
    "    Parameters:\n",
    "    language_code (str): The two-letter ISO 639-1 language code.\n",
    "    \n",
    "    Returns:\n",
    "    str: The full name of the language or a message indicating the code was not found.\n",
    "    \"\"\"\n",
    "    if language_code==\"Other languages\":\n",
    "        return language_code\n",
    "    language = pycountry.languages.get(alpha_2=language_code, default=default)\n",
    "    if language != default:\n",
    "        language = language.name\n",
    "    return language\n",
    "\n",
    "\n",
    "def get_country_name(country_code: str, default: str=\"Unknown Country\") -> str:\n",
    "    \"\"\"\n",
    "    Convert a two-letter country code (ISO 3166-1 alpha-2|) to its full country name.\n",
    "    \n",
    "    Parameters:\n",
    "    country_code (str): The two-letter ISO 3166-1 alpha-2 country code.\n",
    "    \n",
    "    Returns:\n",
    "    str: The full name of the country or a message indicating the code was not found.\n",
    "    \"\"\"\n",
    "    country = pycountry.countries.get(alpha_2=country_code, default=default)\n",
    "    if country != default:\n",
    "        country = country.name\n",
    "    return country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server\n",
    "# test_data = fetch_data(QUERY_ALL)\n",
    "# df_reply = fetch_data(QUERY_REPLY, dtype=False).set_index(\"tweet_id\")\n",
    "# Local\n",
    "path =  os.path.join(\n",
    "    os.path.dirname(\n",
    "        os.path.dirname(\n",
    "            os.getcwd()\n",
    "            )\n",
    "        ),\n",
    "    \"data_processed\", \"local_backup.db\")\n",
    "path\n",
    "test_data = get_local_data(QUERY_ALL, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avia_names = set(COMPANY_NAME_TO_ID.values())\n",
    "\n",
    "replies_to_avia_companies_df = test_data.loc[test_data['user_id'].apply(\n",
    "    lambda x: any(x == avia_name for avia_name in avia_names)\n",
    "    )]\n",
    "replies_to_avia_companies_df = replies_to_avia_companies_df.reset_index()\\\n",
    "    .groupby(\"user_id\").count()[['tweet_id']]\\\n",
    "        .sort_values('tweet_id', ascending=False).reset_index()\n",
    "replies_to_avia_companies_df[\"user_id\"] = replies_to_avia_companies_df[\"user_id\"]\\\n",
    "    .apply(lambda user_id: COMPANY_ID_TO_NAME.get(user_id, user_id))\n",
    "replies_to_avia_companies_df = replies_to_avia_companies_df.set_index(\"user_id\")\n",
    "replies_to_avia_companies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Companies' activity and popularity in social media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airlines_popularity = test_data[test_data['user_id']\\\n",
    "    .apply(lambda x: any(x == avia_name for avia_name in avia_names))]\\\n",
    "        .groupby(\"user_id\")\\\n",
    "            .agg(\n",
    "                retweet_count=(\"retweet_count\", \"sum\"),\n",
    "                favorite_count=(\"favorite_count\", \"sum\"),\n",
    "                reply_count=(\"reply_count\", \"sum\"),\n",
    "                quote_count=(\"quote_count\", \"sum\"),\n",
    "            )\n",
    "df_airlines_popularity.index = df_airlines_popularity.index.map(\n",
    "    lambda user_id: COMPANY_ID_TO_NAME[user_id]\n",
    "    )\n",
    "df_airlines_popularity.index.name = \"Airlines\"\n",
    "df_airlines_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information regarding users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = test_data.groupby(\"user_id\")\n",
    "df_users = df_users.agg(\n",
    "    user_creation_time=(\"user_creation_time\", \"min\"),\n",
    "    verified=(\"verified\", \"min\"),\n",
    "    followers_count=(\"followers_count\", \"min\"),\n",
    "    friends_count=(\"friends_count\", \"min\"),\n",
    "    statuses_count=(\"statuses_count\", \"min\"),\n",
    "    default_profile=(\"default_profile\", \"min\"),\n",
    "    default_profile_image=(\"default_profile_image\", \"max\"),\n",
    "    first_tweet=(\"tweet_creation_time\", \"min\"),\n",
    "    last_tweet=(\"tweet_creation_time\", \"max\"),\n",
    "    possibly_sensitive=(\"possibly_sensitive\", \"sum\"),\n",
    "    favorite_count=(\"favorite_count\", \"sum\"),\n",
    "    retweet_count=(\"retweet_count\", \"sum\"),\n",
    "    reply_count=(\"reply_count\", \"sum\"),\n",
    "    quote_count=(\"quote_count\", \"sum\"),\n",
    "    lang=(\"lang\", \"first\")\n",
    ")\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom user \"trustworthiness\" classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_special = test_data.sort_values(\"tweet_creation_time\", ascending=False)[[\"user_id\", \"replied_tweet_id\", \"quoted_status_id\"]]\n",
    "convo_special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = defaultdict(TrieNode)\n",
    "        self.is_end = False\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, conversation):\n",
    "        node = self.root\n",
    "        for tweet_id in conversation:\n",
    "            node = node.children[tweet_id]\n",
    "        node.is_end = True\n",
    "\n",
    "    def is_subset(self, conversation):\n",
    "        node = self.root\n",
    "        for tweet_id in conversation:\n",
    "            if tweet_id not in node.children:\n",
    "                return False\n",
    "            node = node.children[tweet_id]\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "def trace_conversation(start_tweet_id, tweet_dict):\n",
    "    convo = []\n",
    "    current_tweet_id = start_tweet_id\n",
    "    users_in_conversation = set()\n",
    "    local_processed_tweet_ids = set()  # Local set to track the current conversation\n",
    "    while current_tweet_id:\n",
    "        if current_tweet_id not in tweet_dict or current_tweet_id in local_processed_tweet_ids:\n",
    "            break\n",
    "        tweet_info = tweet_dict[current_tweet_id]\n",
    "        convo.append(current_tweet_id)\n",
    "        users_in_conversation.add(tweet_info['user_id'])\n",
    "        local_processed_tweet_ids.add(current_tweet_id)\n",
    "        if len(users_in_conversation) > 2:\n",
    "            return None  # More than two users, not an exclusive conversation\n",
    "        current_tweet_id = tweet_info['replied_tweet_id']\n",
    "    return convo[::-1] if len(users_in_conversation) == 2 else None\n",
    "\n",
    "\n",
    "def extract_and_filter_conversations(df):\n",
    "    tweet_dict = df.to_dict('index')\n",
    "    conversations = []\n",
    "    trie = Trie()  # Initialize trie for subset checks\n",
    "\n",
    "    # Start tracing conversations from tweets that are replies\n",
    "    for tweet_id in tqdm(df[df['replied_tweet_id'].notnull()].index,\n",
    "                         desc=\"Extracting all conversations\"):\n",
    "        if conversation := trace_conversation(tweet_id, tweet_dict):\n",
    "            if not trie.is_subset(conversation):\n",
    "                trie.insert(conversation)\n",
    "                conversations.append(conversation)\n",
    "\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = extract_and_filter_conversations(convo_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for convo_num, convo in enumerate(conversations, start=1):\n",
    "    data.extend((convo_num, tweet_id) for tweet_id in convo)\n",
    "# Create a DataFrame\n",
    "df_conversations = pd.DataFrame(data, columns=['Conversation', 'Tweet_ID'])\n",
    "\n",
    "# Set MultiIndex\n",
    "df_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge the conversation DataFrame with the test_data DataFrame\n",
    "df_conversations_full = df_conversations.merge(test_data, left_on='Tweet_ID', right_index=True, how='left')\n",
    "\n",
    "# Set the MultiIndex again with Conversation and Tweet_ID\n",
    "df_conversations_full.set_index(['Conversation', 'Tweet_ID'], inplace=True)\n",
    "df_conversations_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_conversation = df_conversations_full.loc[df_conversations_full.index.get_level_values('Conversation').isin(df_conversations_full[df_conversations_full['user_id'] == COMPANY_NAME_TO_ID[\"Lufthansa\"]].index.get_level_values('Conversation'))]\n",
    "airline_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_conversation = airline_conversation.reset_index()\n",
    "airline_conversation['New_Conversation'] = pd.factorize(airline_conversation['Conversation'])[0] + 1\n",
    "airline_conversation = airline_conversation.set_index(['New_Conversation', 'Tweet_ID'])\n",
    "airline_conversation = airline_conversation.sort_index(level='New_Conversation')\n",
    "airline_conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the error message\n",
    "# transformers_logging.set_verbosity_error()\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "#Load the tokenizer and model once\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "#Check if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    device = '/GPU:0'\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "\n",
    "#Set the labels for sentiment results\n",
    "labels = {\n",
    "    0 : 'negative',\n",
    "    1 : 'neutral',\n",
    "    2 : 'positive'\n",
    "}\n",
    "\n",
    "def process_batch(texts):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to a batch of texts using a pre-trained transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list): A list of texts to analyze.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of ranked sentiment labels for the input texts.\n",
    "\n",
    "    This function uses a pre-trained transformer model for sequence classification to analyze the sentiment of the texts in the given batch. It returns a list of ranked sentiment labels for the input texts, where the first label is the most likely sentiment and the subsequent labels are less likely sentiments.\n",
    "    \"\"\"\n",
    "    encoded_input = tokenizer(texts, return_tensors='tf', padding=True, truncation=True)\n",
    "    with tf.device(device):\n",
    "        output = model(encoded_input)\n",
    "    scores = output[0].numpy()\n",
    "    scores = softmax(scores, axis=1)\n",
    "    rankings = np.argsort(scores, axis=1)[:, ::-1]\n",
    "    return [labels[rank[0]] for rank in rankings]\n",
    "\n",
    "def apply_sentiment_analysis(df, text_column, batch_size=64, max_workers=14):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to the given DataFrame using a pre-trained transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the text column to analyze.\n",
    "    text_column (str): The name of the column in the DataFrame containing the text to analyze.\n",
    "    batch_size (int): The number of texts to process in each batch. Default is 128.\n",
    "    max_workers (int): The maximum number of worker threads to use for parallel processing. Default is 4.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The input DataFrame with an additional 'sentiment' column containing the sentiment analysis results.\n",
    "\n",
    "    This function uses a pre-trained transformer model for sequence classification to analyze the sentiment of the texts in the given DataFrame. It applies the sentiment analysis in parallel using multiple worker threads to improve performance. The results are then added to the input DataFrame as a new 'sentiment' column.\n",
    "    \"\"\"\n",
    "    texts = df[text_column].tolist()\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            results.extend(executor.submit(process_batch, batch).result())\n",
    "\n",
    "    df['sentiment'] = results\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  \n",
    "full_text_df = airline_conversation[['full_text']].copy()\n",
    "airline_conversation_sample = full_text_df.head(100)\n",
    "airline_conversation_sample = apply_sentiment_analysis(airline_conversation_sample, 'full_text')\n",
    "airline_conversation_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
