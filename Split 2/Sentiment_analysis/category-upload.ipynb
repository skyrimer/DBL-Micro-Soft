{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8588297,"sourceType":"datasetVersion","datasetId":5136945},{"sourceId":8588486,"sourceType":"datasetVersion","datasetId":5137050},{"sourceId":8602919,"sourceType":"datasetVersion","datasetId":5147408},{"sourceId":8648426,"sourceType":"datasetVersion","datasetId":5180182}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport sqlite3\nimport pandas as pd\nimport tensorflow as tf\nfrom transformers import pipeline, XLMRobertaTokenizer, AutoTokenizer, TFAutoModelForSequenceClassification\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom typing import List, Tuple\nfrom scipy.special import softmax\nfrom sqlalchemy import create_engine\nimport re\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:07:52.668833Z","iopub.execute_input":"2024-06-10T08:07:52.669206Z","iopub.status.idle":"2024-06-10T08:08:12.235237Z","shell.execute_reply.started":"2024-06-10T08:07:52.669176Z","shell.execute_reply":"2024-06-10T08:08:12.233699Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-10 08:07:54.942564: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-10 08:07:54.942688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-10 08:07:55.090637: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Constants","metadata":{}},{"cell_type":"code","source":"# USER, DATABASE = \"nezox2um_test\", \"nezox2um_test\"\nQUERY_ALL = \"\"\"\nSELECT T.tweet_id, T.full_text\nFROM Tweets T\nINNER JOIN Conversations C\nON T.tweet_id = C.first_tweet_id\nWHERE C.category IS NULL;\n\"\"\"\n#QUERY_ALL = \"\"\"\n#SELECT c.conversation_id, c.tweet_id, t.full_text\n#FROM Conversations c\n#INNER JOIN Tweets t ON c.tweet_id = t.tweet_id\n#INNER JOIN ConversationsCategory cc ON c.conversation_id = cc.conversation_id\n#WHERE c.tweet_order = 1 AND cc.category = 'No category';\n#\"\"\"\n\nDTYPES = {\n\"tweet_id\": \"object\",\n\"full_text\": \"object\",\n}\n\nCOMPANY_NAME_TO_ID = {\n    \"Klm\": \"56377143\",\n    \"Air France\": \"106062176\",\n    \"British Airways\": \"18332190\",\n    \"American Air\": \"22536055\",\n    \"Lufthansa\": \"124476322\",\n    \"Air Berlin\": \"26223583\",\n    \"Air Berlin assist\": \"2182373406\",\n    \"easyJet\": \"38676903\",\n    \"Ryanair\": \"1542862735\",\n    \"Singapore Airlines\": \"253340062\",\n    \"Qantas\": \"218730857\",\n    \"Etihad Airways\": \"45621423\",\n    \"Virgin Atlantic\": \"20626359\",\n}\n\nCOMPANY_ID_TO_NAME = {v: k for k, v in COMPANY_NAME_TO_ID.items()}","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:24:11.722754Z","iopub.execute_input":"2024-06-09T14:24:11.723676Z","iopub.status.idle":"2024-06-09T14:24:11.731908Z","shell.execute_reply.started":"2024-06-09T14:24:11.723624Z","shell.execute_reply":"2024-06-09T14:24:11.730715Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions","metadata":{}},{"cell_type":"code","source":"def get_local_data(query: str, path: str) -> pd.DataFrame:\n    # Connect to the SQLite database using a context manager\n    with sqlite3.connect(path) as connection:\n        return pd.read_sql_query(query, connection,\n                                   dtype=DTYPES,\n                                   index_col='tweet_id')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T13:59:46.230394Z","iopub.execute_input":"2024-06-04T13:59:46.230853Z","iopub.status.idle":"2024-06-04T13:59:46.237659Z","shell.execute_reply.started":"2024-06-04T13:59:46.230816Z","shell.execute_reply":"2024-06-04T13:59:46.236193Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Loading","metadata":{}},{"cell_type":"code","source":"!cp \"/kaggle/input/pre-cat/local_backup.db\" \"/kaggle/working/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path =  \"/kaggle/working/local_backup.db\"\n\ntest_data = get_local_data(QUERY_ALL, path)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:00:07.024846Z","iopub.execute_input":"2024-06-04T14:00:07.025337Z","iopub.status.idle":"2024-06-04T14:02:01.345391Z","shell.execute_reply.started":"2024-06-04T14:00:07.025298Z","shell.execute_reply":"2024-06-04T14:02:01.344212Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:02:01.347716Z","iopub.execute_input":"2024-06-04T14:02:01.348142Z","iopub.status.idle":"2024-06-04T14:02:01.367224Z","shell.execute_reply.started":"2024-06-04T14:02:01.348085Z","shell.execute_reply":"2024-06-04T14:02:01.365905Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                             full_text\ntweet_id                                                              \n1244693824519184392  We in #Houston said goodbye to @KLM’s #QueenOf...\n1244550514970329088  @Grenzmauer75 @elliotday @easyJet Exactly. Do ...\n1243532085131743232  @FlySWISS @British_Airways @qatarairways Thank...\n1244683000195022855  @RosamariaP3 Hola Rosa ✌ Siento que aún no hay...\n1244691623289724928  @xdarc79 @hoetschenreuter @flavioArCab @Chapux...\n...                                                                ...\n491999025974218752      @Ryanair What if I make it into a Turban then?\n452656989685178368   @AmericanAir Please help me!!  I've fallen on ...\n451124070730719233   @AmericanAir i was kidding thanks for the foll...\n430790355962052608   @AmericanAir phew, they finally turned on the ...\n248528541157834752   Un-fucking believable!\\nThanks @BritishAirways...\n\n[1063960 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text</th>\n    </tr>\n    <tr>\n      <th>tweet_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1244693824519184392</th>\n      <td>We in #Houston said goodbye to @KLM’s #QueenOf...</td>\n    </tr>\n    <tr>\n      <th>1244550514970329088</th>\n      <td>@Grenzmauer75 @elliotday @easyJet Exactly. Do ...</td>\n    </tr>\n    <tr>\n      <th>1243532085131743232</th>\n      <td>@FlySWISS @British_Airways @qatarairways Thank...</td>\n    </tr>\n    <tr>\n      <th>1244683000195022855</th>\n      <td>@RosamariaP3 Hola Rosa ✌ Siento que aún no hay...</td>\n    </tr>\n    <tr>\n      <th>1244691623289724928</th>\n      <td>@xdarc79 @hoetschenreuter @flavioArCab @Chapux...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>491999025974218752</th>\n      <td>@Ryanair What if I make it into a Turban then?</td>\n    </tr>\n    <tr>\n      <th>452656989685178368</th>\n      <td>@AmericanAir Please help me!!  I've fallen on ...</td>\n    </tr>\n    <tr>\n      <th>451124070730719233</th>\n      <td>@AmericanAir i was kidding thanks for the foll...</td>\n    </tr>\n    <tr>\n      <th>430790355962052608</th>\n      <td>@AmericanAir phew, they finally turned on the ...</td>\n    </tr>\n    <tr>\n      <th>248528541157834752</th>\n      <td>Un-fucking believable!\\nThanks @BritishAirways...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1063960 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Airlines list\nairlines = [\n    'KLM', \"British_Airways\", \"airfrance\", \"AmericanAir\", \"lufthansa\", \n    \"airberlinAssist\", \"easyJet\", \"Ryanair\", \"SingaporeAir\", \"Qantas\",\n    \"EtihadAirways\", \"VirginAtlantic\", \"airberlin\"\n]\n\n# Function to clean mentions not in the airlines list\ndef clean_mentions(text):\n    # Regex pattern to find mentions\n    mention_pattern = r'@([A-Za-z0-9_]+)'\n    # Regex pattern to find URLs\n    url_pattern = r'https?://\\S+|www\\.\\S+'\n    rt_pattern = r'^RT\\s+'\n    \n    # Substitute mentions with an empty string\n    text = re.sub(mention_pattern, '', text)\n    # Substitute URLs with an empty string\n    text = re.sub(url_pattern, '', text)\n    text = re.sub(rt_pattern, \"\", text)\n    \n    return text.strip()\n\n# Apply the cleaning function to the DataFrame\ntest_data['cleaned_text'] = test_data['full_text'].apply(clean_mentions)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:02:20.852289Z","iopub.execute_input":"2024-06-04T14:02:20.853224Z","iopub.status.idle":"2024-06-04T14:02:34.540428Z","shell.execute_reply.started":"2024-06-04T14:02:20.853181Z","shell.execute_reply":"2024-06-04T14:02:34.538899Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:02:34.542289Z","iopub.execute_input":"2024-06-04T14:02:34.542647Z","iopub.status.idle":"2024-06-04T14:02:34.558304Z","shell.execute_reply.started":"2024-06-04T14:02:34.542619Z","shell.execute_reply":"2024-06-04T14:02:34.556761Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                             full_text  \\\ntweet_id                                                                 \n1244693824519184392  We in #Houston said goodbye to @KLM’s #QueenOf...   \n1244550514970329088  @Grenzmauer75 @elliotday @easyJet Exactly. Do ...   \n1243532085131743232  @FlySWISS @British_Airways @qatarairways Thank...   \n1244683000195022855  @RosamariaP3 Hola Rosa ✌ Siento que aún no hay...   \n1244691623289724928  @xdarc79 @hoetschenreuter @flavioArCab @Chapux...   \n...                                                                ...   \n491999025974218752      @Ryanair What if I make it into a Turban then?   \n452656989685178368   @AmericanAir Please help me!!  I've fallen on ...   \n451124070730719233   @AmericanAir i was kidding thanks for the foll...   \n430790355962052608   @AmericanAir phew, they finally turned on the ...   \n248528541157834752   Un-fucking believable!\\nThanks @BritishAirways...   \n\n                                                          cleaned_text  \ntweet_id                                                                \n1244693824519184392  We in #Houston said goodbye to ’s #QueenOfTheS...  \n1244550514970329088  Exactly. Do they plan to go bankrupt, pay thei...  \n1243532085131743232  Thanks all for still flying. We are on our way...  \n1244683000195022855  Hola Rosa ✌ Siento que aún no hayas recibido e...  \n1244691623289724928  Tal cual y mucho de ellos ya en plena pandemia...  \n...                                                                ...  \n491999025974218752               What if I make it into a Turban then?  \n452656989685178368   Please help me!!  I've fallen on one of your p...  \n451124070730719233             i was kidding thanks for the follow tho  \n430790355962052608   phew, they finally turned on the air... Wait, ...  \n248528541157834752   Un-fucking believable!\\nThanks  the grief and ...  \n\n[1063960 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text</th>\n      <th>cleaned_text</th>\n    </tr>\n    <tr>\n      <th>tweet_id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1244693824519184392</th>\n      <td>We in #Houston said goodbye to @KLM’s #QueenOf...</td>\n      <td>We in #Houston said goodbye to ’s #QueenOfTheS...</td>\n    </tr>\n    <tr>\n      <th>1244550514970329088</th>\n      <td>@Grenzmauer75 @elliotday @easyJet Exactly. Do ...</td>\n      <td>Exactly. Do they plan to go bankrupt, pay thei...</td>\n    </tr>\n    <tr>\n      <th>1243532085131743232</th>\n      <td>@FlySWISS @British_Airways @qatarairways Thank...</td>\n      <td>Thanks all for still flying. We are on our way...</td>\n    </tr>\n    <tr>\n      <th>1244683000195022855</th>\n      <td>@RosamariaP3 Hola Rosa ✌ Siento que aún no hay...</td>\n      <td>Hola Rosa ✌ Siento que aún no hayas recibido e...</td>\n    </tr>\n    <tr>\n      <th>1244691623289724928</th>\n      <td>@xdarc79 @hoetschenreuter @flavioArCab @Chapux...</td>\n      <td>Tal cual y mucho de ellos ya en plena pandemia...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>491999025974218752</th>\n      <td>@Ryanair What if I make it into a Turban then?</td>\n      <td>What if I make it into a Turban then?</td>\n    </tr>\n    <tr>\n      <th>452656989685178368</th>\n      <td>@AmericanAir Please help me!!  I've fallen on ...</td>\n      <td>Please help me!!  I've fallen on one of your p...</td>\n    </tr>\n    <tr>\n      <th>451124070730719233</th>\n      <td>@AmericanAir i was kidding thanks for the foll...</td>\n      <td>i was kidding thanks for the follow tho</td>\n    </tr>\n    <tr>\n      <th>430790355962052608</th>\n      <td>@AmericanAir phew, they finally turned on the ...</td>\n      <td>phew, they finally turned on the air... Wait, ...</td>\n    </tr>\n    <tr>\n      <th>248528541157834752</th>\n      <td>Un-fucking believable!\\nThanks @BritishAirways...</td>\n      <td>Un-fucking believable!\\nThanks  the grief and ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1063960 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Categorization","metadata":{}},{"cell_type":"markdown","source":"### Prepare the data for training and testing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\n\n# Step 1: Load the data\ndata = pd.read_csv('/kaggle/input/data-with-labels/translated_and_categorized_cleaned - Sheet1.csv')\ndata = data[150:]\n# Step 2: Preprocess the text data\ndata['text'] = data['text'].str.lower()\n\n# Step 3: Convert text data to numerical format using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\nX = tfidf_vectorizer.fit_transform(data['text']).toarray()\ny = data['Category']\n\n# Step 4: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 5: Encode the labels\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\n# Step 6: Create and train the SVM model with a linear kernel\nsvm_model = SVC(kernel='linear', probability=True, random_state=42)\nsvm_model.fit(X_train, y_train_encoded)\n\n# Step 7: Evaluate the model performance on the test set\ny_pred = svm_model.predict(X_test)\n\n# Ensuring the target names match the unique classes in the training set\nunique_classes = label_encoder.classes_\nprint(classification_report(y_test_encoded, y_pred, labels=range(len(unique_classes)), target_names=unique_classes))\n\ndef predict_category(tweet, vectorizer, model, label_encoder):\n    tweet_processed = tweet.lower()\n    tweet_vectorized = vectorizer.transform([tweet_processed]).toarray()\n    predicted_label = model.predict(tweet_vectorized)\n    category = label_encoder.inverse_transform(predicted_label)\n    return category[0]\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:35:40.849469Z","iopub.execute_input":"2024-06-10T08:35:40.850308Z","iopub.status.idle":"2024-06-10T08:35:43.754337Z","shell.execute_reply.started":"2024-06-10T08:35:40.850269Z","shell.execute_reply":"2024-06-10T08:35:43.753331Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"                                 precision    recall  f1-score   support\n\n                 baggage issues       1.00      0.25      0.40         4\n               booking problems       0.67      0.20      0.31        10\n              check-in troubles       0.00      0.00      0.00         1\n    customer service complaints       1.00      0.20      0.33         5\nflight delays and cancellations       1.00      0.71      0.83         7\n    flight information requests       1.00      0.20      0.33         5\n   food and beverage complaints       0.00      0.00      0.00         0\n           in-flight experience       0.55      0.98      0.71        47\n                   lost luggage       1.00      0.25      0.40         4\n     promotion and offer issues       1.00      0.57      0.73         7\n              refund complaints       1.00      0.67      0.80         3\n   safety and security concerns       0.00      0.00      0.00         0\nseating and boarding challenges       0.75      0.25      0.38        12\n    special assistance requests       1.00      1.00      1.00         1\n         technical difficulties       0.00      0.00      0.00         0\n\n                      micro avg       0.63      0.63      0.63       106\n                      macro avg       0.66      0.35      0.41       106\n                   weighted avg       0.73      0.63      0.58       106\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/data-with-labels/translated_and_categorized_cleaned - Sheet1.csv\")\ntest = df.head(100)\ntest['our_guess'] = test['text'].apply(lambda x: predict_category(x, tfidf_vectorizer, svm_model, label_encoder))\naccuracy = (test['Category'] == test['our_guess']).mean()*100\n\n\nprint(test)\nprint(f\"Accuracy after fine-tuning: {accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:36:54.338951Z","iopub.execute_input":"2024-06-10T08:36:54.339374Z","iopub.status.idle":"2024-06-10T08:36:54.521512Z","shell.execute_reply.started":"2024-06-10T08:36:54.339343Z","shell.execute_reply":"2024-06-10T08:36:54.520398Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"                                                 text  \\\n0   RT @sandeeprrao1991: BREAKING:-\\nKLM to fly 3x...   \n1   Thanks @British_Airways I really needed the ex...   \n2   So @AmericanAir @EWRairport lied to an old lad...   \n3   And had to pay extra £50 because bag wouldnt f...   \n4   @AmericanAir, I should be on a flight to LA, i...   \n..                                                ...   \n95  @SingaporeAir Similar to what Emirates offers ...   \n96  5* Lufthansa SALE: Cheap flights from Germany ...   \n97  RT @asemota: Why are the @British_Airways plan...   \n98  RT @AllDayIDreamOf: .@OonaDahl Dahl and Newman...   \n99  Not the first time I'm seeing this complaint. ...   \n\n                                          translation  \\\n0   RT @sandeeprrao1991: BREAKING:-\\nKLM to fly 3x...   \n1   Thanks @British_Airways I really needed the ex...   \n2   So @AmericanAir @EWRairport lied to an old lad...   \n3   And had to pay extra £50 because bag wouldnt f...   \n4   @AmericanAir, I should be on a flight to LA, i...   \n..                                                ...   \n95  @SingaporeAir Similar to what Emirates offers ...   \n96  5* Lufthansa SALE: Cheap flights from Germany ...   \n97  RT @asemota: Why are the @British_Airways plan...   \n98  RT @AllDayIDreamOf: .@OonaDahl Dahl and Newman...   \n99  Not the first time I'm seeing this complaint. ...   \n\n                           Category                    our_guess  \n0       flight information requests  flight information requests  \n1   flight delays and cancellations         in-flight experience  \n2              in-flight experience         in-flight experience  \n3   flight delays and cancellations         in-flight experience  \n4              in-flight experience         in-flight experience  \n..                              ...                          ...  \n95       promotion and offer issues         in-flight experience  \n96             in-flight experience         in-flight experience  \n97                refund complaints            refund complaints  \n98  seating and boarding challenges         in-flight experience  \n99      customer service complaints         in-flight experience  \n\n[100 rows x 4 columns]\nAccuracy after fine-tuning: 61.00%\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/3133944191.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test['our_guess'] = test['text'].apply(lambda x: predict_category(x, tfidf_vectorizer, svm_model, label_encoder))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Fine-tuning the model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\ndf_train = df.head[100:].to_dict(orient='records')\n\nmodel_name = 'MoritzLaurer/multilingual-MiniLMv2-L6-mnli-xnli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=len(candidate_labels), \n    ignore_mismatched_sizes=True\n)\n\nfor layer in model.layers:\n    layer.trainable = True\n\ntexts = [item['text'] for item in df_train]\ntokenized_inputs = tokenizer(texts, padding=True, truncation=True, max_length = 512 , return_tensors=\"tf\")\n\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform([item['Category'] for item in df_train])\nlabels = tf.convert_to_tensor(labels, dtype=tf.int32)\n\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    {'input_ids': tokenized_inputs['input_ids'], 'attention_mask': tokenized_inputs['attention_mask']},\n    labels,\n    epochs=5,\n    batch_size=16,\n    validation_split=0.1,  # Add validation split to monitor overfitting\n    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)]\n)\n\nmodel.save_pretrained(\"/kaggle/working/fine_tuned_classification\")\ntokenizer.save_pretrained(\"/kaggle/working/fine_tuned_tokenizer\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:36:50.928763Z","iopub.execute_input":"2024-06-09T18:36:50.929231Z","iopub.status.idle":"2024-06-09T18:45:07.454635Z","shell.execute_reply.started":"2024-06-09T18:36:50.929193Z","shell.execute_reply":"2024-06-09T18:45:07.453343Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaForSequenceClassification: ['classifier.out_proj.weight', 'roberta.embeddings.position_ids', 'classifier.out_proj.bias']\n- This IS expected if you are initializing TFXLMRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLMRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFXLMRobertaForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForSequenceClassification for predictions without further training.\nSome weights of TFXLMRobertaForSequenceClassification were not initialized from the model checkpoint are newly initialized because the shapes did not match:\n- classifier.out_proj.weight: found shape (3, 384) in the checkpoint and (384, 17) in the model instantiated\n- classifier.out_proj.bias: found shape (3,) in the checkpoint and (17,) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n36/36 [==============================] - 133s 3s/step - loss: 2.7476 - accuracy: 0.2796 - val_loss: 2.6341 - val_accuracy: 0.4444\nEpoch 2/5\n36/36 [==============================] - 89s 2s/step - loss: 2.5950 - accuracy: 0.4212 - val_loss: 2.5137 - val_accuracy: 0.4444\nEpoch 3/5\n36/36 [==============================] - 89s 2s/step - loss: 2.4883 - accuracy: 0.4212 - val_loss: 2.4181 - val_accuracy: 0.4444\nEpoch 4/5\n36/36 [==============================] - 88s 2s/step - loss: 2.3930 - accuracy: 0.4212 - val_loss: 2.3334 - val_accuracy: 0.4444\nEpoch 5/5\n36/36 [==============================] - 88s 2s/step - loss: 2.2996 - accuracy: 0.4301 - val_loss: 2.2481 - val_accuracy: 0.4444\n","output_type":"stream"},{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/fine_tuned_tokenizer/tokenizer_config.json',\n '/kaggle/working/fine_tuned_tokenizer/special_tokens_map.json',\n '/kaggle/working/fine_tuned_tokenizer/sentencepiece.bpe.model',\n '/kaggle/working/fine_tuned_tokenizer/added_tokens.json',\n '/kaggle/working/fine_tuned_tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\nimport pandas as pd\n\n# Define the candidate labels\ncandidate_labels = [\n    \"flight delays and cancellations\",\n    \"booking problems\",\n    \"check-in troubles\",\n    \"customer service complaints\",\n    \"seating and boarding challenges\",\n    \"in-flight experience\",\n    \"flight information requests\",\n    \"refund complaints\",\n    \"frequent flyer concerns\",\n    \"safety and security concerns\",\n    \"special assistance requests\",\n    \"food and beverage complaints\",\n    \"overbooking complaints\",\n    \"technical difficulties\",\n    \"promotion and offer issues\",\n    \"lost luggage\",\n    \"baggage issues\"\n]\n\n# Load your pre-trained model and tokenizer using the pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/multilingual-MiniLMv2-L6-mnli-xnli\")\n\n# Assuming 'df' is your DataFrame containing test data\ndf_test = df.head(100).to_dict(orient='records')\n\n# Evaluate on test data\ntest_texts = [item['text'] for item in df_test]\ntest_labels = [item['Category'] for item in df_test]\n\n# Perform predictions\npredictions = classifier(test_texts, candidate_labels=candidate_labels, multi_label=False)\n\n# Extract predicted labels\npredicted_labels = [pred['labels'][0] for pred in predictions]\n\n# Encode true and predicted labels to indices\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(candidate_labels)\ntest_labels_encoded = label_encoder.transform(test_labels)\npredicted_labels_encoded = label_encoder.transform(predicted_labels)\n\n# Calculate accuracy\naccuracy = (predicted_labels_encoded == test_labels_encoded).mean() * 100\nprint(f\"Accuracy before fine-tuning: {accuracy:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:31:59.214791Z","iopub.execute_input":"2024-06-10T08:31:59.215252Z","iopub.status.idle":"2024-06-10T08:32:35.541206Z","shell.execute_reply.started":"2024-06-10T08:31:59.215222Z","shell.execute_reply":"2024-06-10T08:32:35.540083Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Accuracy before fine-tuning: 19.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nimport tensorflow as tf\nimport pandas as pd\n\ncandidate_labels = [\n    \"flight delays and cancellations\",\n    \"booking problems\",\n    \"check-in troubles\",\n    \"customer service complaints\",\n    \"seating and boarding challenges\",\n    \"in-flight experience\",\n    \"flight information requests\",\n    \"refund complaints\",\n    \"frequent flyer concerns\",\n    \"safety and security concerns\",\n    \"special assistance requests\",\n    \"food and beverage complaints\",\n    \"overbooking complaints\",\n    \"technical difficulties\",\n    \"promotion and offer issues\",\n    \"lost luggage\",\n    \"baggage issues\"\n]\n\n# Load your fine-tuned model and tokenizer\nmodel_name = '/kaggle/working/fine_tuned_classification'\ntokenizer_name = '/kaggle/working/fine_tuned_tokenizer'\n\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=len(candidate_labels), \n    ignore_mismatched_sizes=True\n)\n\n# Assuming 'df' is your DataFrame containing test data\ndf_test = df.to_dict(orient='records')\ndf_test = df_test[:100]\n\n# Evaluate on test data\ntest_texts = [item['text'] for item in df_test]\ntokenized_test_inputs = tokenizer(test_texts, padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n\ntest_labels = label_encoder.transform([item['Category'] for item in df_test])\ntest_labels = tf.convert_to_tensor(test_labels, dtype=tf.int32)\n\n# Perform predictions\npredictions = model.predict({'input_ids': tokenized_test_inputs['input_ids'], 'attention_mask': tokenized_test_inputs['attention_mask']})\npredicted_labels = tf.argmax(predictions.logits, axis=1).numpy()\n\n# Calculate accuracy\naccuracy = (predicted_labels == test_labels.numpy()).mean()*100\nprint(f\"Accuracy after fine-tuning: {accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:32:57.175801Z","iopub.execute_input":"2024-06-10T08:32:57.176628Z","iopub.status.idle":"2024-06-10T08:33:08.799174Z","shell.execute_reply.started":"2024-06-10T08:32:57.176591Z","shell.execute_reply":"2024-06-10T08:33:08.798176Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n\nAll the layers of TFXLMRobertaForSequenceClassification were initialized from the model checkpoint at /kaggle/working/fine_tuned_classification.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForSequenceClassification for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"4/4 [==============================] - 8s 305ms/step\nAccuracy after fine-tuning: 0.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = '/kaggle/working/fine_tuned_classification'\ntokenizer_name = '/kaggle/working/fine_tuned_tokenizer'\n\n# Check GPU availability\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\ngpus = tf.config.list_logical_devices('GPU')\nstrategy = tf.distribute.MirroredStrategy()\n\n# Initialize tokenizer and pipeline\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n\nprint(f\"Maximum sequence length for the tokenizer: {tokenizer.model_max_length}\")\n\ndevice = 0 if tf.config.experimental.list_physical_devices('GPU') else -1\nprint(device)\nclassifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/multilingual-MiniLMv2-L6-mnli-xnli\", device=device)\n\n#USE THIS WHEN MODEL IS NORMALLY FINE-TUNED\n#classifier = pipeline(\"zero-shot-classification\", model= model, tokenizer = tokenizer, device=device)\n\ncandidate_labels = [\n    \"flight delays and cancellations\",\n    \"booking problems\",\n    \"check-in troubles\",\n    \"customer service complaints\",\n    \"seating and boarding challenges\",\n    \"in-flight experience\",\n    \"flight information requests\",\n    \"refund complaints\",\n    \"frequent flyer concerns\",\n    \"safety and security concerns\",\n    \"special assistance requests\",\n    \"food and beverage complaints\",\n    \"overbooking complaints\",\n    \"technical difficulties\",\n    \"promotion and offer issues\",\n    \"lost luggage\",\n    \"baggage issues\"\n]\n\ndef classify_batch_conversations(conversations):\n    if not conversations:  # Ensure there are conversations to process\n        return [], []\n    results = classifier(conversations, candidate_labels)\n    labels = [result['labels'][0] for result in results]\n    scores = [result['scores'][0] for result in results]\n    return labels, scores\n\ndef update_text_local(batch: List[Tuple[str, str]], db_path: str) -> None:\n    connection = connect_to_local_database(db_path)\n    if connection is None:\n        return\n    try:\n        cursor = connection.cursor()\n        update_query = \"UPDATE Conversations SET category = ? WHERE first_tweet_id = ?\"\n        cursor.executemany(update_query, batch)\n        connection.commit()\n    except sqlite3.Error as e:\n        print(f\"Error updating batch: {e}\")\n    finally:\n        if cursor:\n            cursor.close()\n        if connection:\n            connection.close()\n\ndef process_batch(batch: List[str]):\n    if not batch:  # Ensure there are texts to process\n        return []\n    labels, scores = classify_batch_conversations(batch)\n    return labels, scores\n\ndef clear_gpu_memory():\n    tf.keras.backend.clear_session()\n    try:\n        tf.compat.v1.reset_default_graph()\n    except AttributeError:\n        pass\n    gc.collect()\n\ndef apply_sentiment_analysis(df, text_column, batch_size=128, max_workers=4):\n    texts = df[text_column].tolist()\n    labels = []\n    scores = []    \n\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i : i + batch_size]\n            if batch:  # Ensure the batch is not empty\n                batch_labels, batch_scores = executor.submit(process_batch, batch).result()\n                labels.extend(batch_labels)\n                scores.extend(batch_scores)\n                clear_gpu_memory()\n    df[\"category\"] = labels\n    df[\"confidence\"] = scores\n    return df\n\ndef get_batches(df: pd.DataFrame, batch_size: int = 1000) -> List[pd.DataFrame]:\n    return [df.iloc[i : i + batch_size] for i in range(0, len(df), batch_size)]\n\ndef convert_to_list(df: pd.DataFrame) -> List[List]:\n    tweet_ids = df.index.to_numpy()\n    sentiments = df[\"category\"].to_numpy()\n    return list(zip(sentiments, tweet_ids))\n\ndef connect_to_local_database(db_path: str):\n    try:\n        return sqlite3.connect(db_path)\n    except sqlite3.Error as e:\n        print(f\"Error while connecting to SQLite: {e}\")\n    return None\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:32:56.078586Z","iopub.status.idle":"2024-06-09T17:32:56.079107Z","shell.execute_reply.started":"2024-06-09T17:32:56.078862Z","shell.execute_reply":"2024-06-09T17:32:56.078883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_copy = test_data.copy()\ntest_data_copy = test_data_copy.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:06:33.291756Z","iopub.execute_input":"2024-06-04T14:06:33.292918Z","iopub.status.idle":"2024-06-04T14:06:33.545504Z","shell.execute_reply.started":"2024-06-04T14:06:33.292871Z","shell.execute_reply":"2024-06-04T14:06:33.543987Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for_batches = test_data_copy[test_data_copy['cleaned_text'].apply(len) >= 1]\ndata_batches = get_batches(for_batches[[\"cleaned_text\"]], 1000)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:06:34.897677Z","iopub.execute_input":"2024-06-04T14:06:34.898123Z","iopub.status.idle":"2024-06-04T14:06:34.907029Z","shell.execute_reply.started":"2024-06-04T14:06:34.898073Z","shell.execute_reply":"2024-06-04T14:06:34.905620Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# for_batches = test_data[test_data['cleaned_text'].apply(len) >= 1]\n# data_batches = get_batches(for_batches[[\"cleaned_text\"]], 1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in tqdm(data_batches, desc=\"Updating text: \"):\n    df_sentiment = apply_sentiment_analysis(batch, \"cleaned_text\", 256, 2)\n    update_text_local(convert_to_list(df_sentiment), path)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:07:06.264715Z","iopub.execute_input":"2024-06-04T14:07:06.265513Z","iopub.status.idle":"2024-06-04T14:07:12.465587Z","shell.execute_reply.started":"2024-06-04T14:07:06.265444Z","shell.execute_reply":"2024-06-04T14:07:12.464424Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Updating text: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"df_sentiment","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}