{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","from concurrent.futures import ThreadPoolExecutor\n","from typing import List, Tuple\n","\n","import gc\n","import re\n","import sqlite3\n","import pandas as pd\n","import tensorflow as tf\n","from scipy.special import softmax\n","from tqdm.notebook import tqdm\n","from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T14:15:36.077957Z","iopub.status.busy":"2024-05-31T14:15:36.077436Z","iopub.status.idle":"2024-05-31T14:15:36.096500Z","shell.execute_reply":"2024-05-31T14:15:36.095418Z","shell.execute_reply.started":"2024-05-31T14:15:36.077926Z"},"trusted":true},"outputs":[],"source":["\n","def fetch_data(query: str, path: str) -> pd.DataFrame:\n","    with sqlite3.connect(path) as connection:\n","        return pd.read_sql_query(query, connection, index_col='tweet_id')\n","\n","\n","def process_batch(texts):\n","    \"\"\"\n","    Apply sentiment analysis to a batch of texts using a pre-trained transformer model.\n","\n","    Parameters:\n","    texts (list): A list of texts to analyze.\n","\n","    Returns:\n","    list: A list of ranked sentiment labels for the input texts.\n","\n","    This function uses a pre-trained transformer model for sequence classification to analyze the sentiment of the texts in the given batch. It returns a list of ranked sentiment labels for the input texts, where the first label is the most likely sentiment and the subsequent labels are less likely sentiments.\n","    \"\"\"\n","    # Get the maximum sequence length for the model\n","\n","    encoded_input = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True)\n","    with tf.device(device):\n","        output = model(encoded_input)\n","\n","    scores = output[0].numpy()\n","    scores = softmax(scores, axis=1)\n","\n","    sentiment_scores = scores[:, 2] - scores[:, 0]\n","    return sentiment_scores.tolist()\n","\n","def clear_gpu_memory():\n","    tf.keras.backend.clear_session()  # Clear the current session\n","    try:\n","        tf.compat.v1.reset_default_graph()  # For TensorFlow 1.x compatibility\n","    except AttributeError:\n","        pass\n","    gc.collect()  # Explicitly call the garbage collector\n","\n","\n","def apply_sentiment_analysis(df, text_column, batch_size=128, max_workers=4):\n","    \"\"\"\n","    Apply sentiment analysis to the given DataFrame using a pre-trained transformer model.\n","\n","    Parameters:\n","    df (pd.DataFrame): The input DataFrame containing the text column to analyze.\n","    text_column (str): The name of the column in the DataFrame containing the text to analyze.\n","    batch_size (int): The number of texts to process in each batch. Default is 128.\n","    max_workers (int): The maximum number of worker threads to use for parallel processing. Default is 4.\n","\n","    Returns:\n","    pd.DataFrame: The input DataFrame with an additional 'sentiment' column containing the sentiment analysis results.\n","\n","    This function uses a pre-trained transformer model for sequence classification to analyze the sentiment of the texts in the given DataFrame. It applies the sentiment analysis in parallel using multiple worker threads to improve performance. The results are then added to the input DataFrame as a new 'sentiment' column.\n","    \"\"\"\n","    texts = df[text_column].tolist()\n","    results = []\n","\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        for i in range(0, len(texts), batch_size):\n","            batch = texts[i : i + batch_size]\n","            results.extend(executor.submit(process_batch, batch).result())\n","            clear_gpu_memory()\n","    df[\"sentiment\"] = results\n","    return df\n","\n","\n","def get_batches(df: pd.DataFrame, batch_size: int = 1000) -> List[pd.DataFrame]:\n","    \"\"\"\n","    Split DataFrame into batches of DataFrames with specified batch size.\n","\n","    Args:\n","        df: The DataFrame containing tweet data.\n","        batch_size: The size of each batch.\n","\n","    Returns:\n","        A list of DataFrames, each containing a batch of rows.\n","    \"\"\"\n","    return [df.iloc[i : i + batch_size] for i in range(0, len(df), batch_size)]\n","\n","def convert_to_list(df: pd.DataFrame) -> List[List]:\n","    \"\"\"\n","    Convert DataFrame with tweet_id as index to a list of lists containing sentiment and tweet_id.\n","\n","    Args:\n","        df: The DataFrame with tweet_id as index and sentiment as a column.\n","\n","    Returns:\n","        A list of lists containing tweet_id and sentiment.\n","    \"\"\"\n","    tweet_ids = df.index.to_numpy()\n","    sentiments = df[\"sentiment\"].to_numpy()\n","    return tuple(zip(sentiments, tweet_ids))\n","\n","def connect_to_local_database(db_path: str):\n","    \"\"\"\n","    Establish a connection to the local SQLite database.\n","\n","    Args:\n","        db_path: The path to the SQLite database file.\n","\n","    Returns:\n","        A connection object to the SQLite database.\n","    \"\"\"\n","    try:\n","        return sqlite3.connect(db_path)\n","    except sqlite3.Error as e:\n","        print(f\"Error while connecting to SQLite: {e}\")\n","    return None\n","\n","\n","def update_text_local(\n","    batch: List[Tuple[str, str]], db_path: str\n",") -> None:\n","    \"\"\"\n","    Update full_text values for a batch of data in the local SQLite database.\n","\n","    Args:\n","        batch: List of (full_text, tweet_id) pairs.\n","        db_path: The path to the SQLite database file.\n","    \"\"\"\n","    connection = connect_to_local_database(db_path)\n","    if connection is None:\n","        return\n","    try:\n","        cursor = connection.cursor()\n","        update_query = \"UPDATE Tweets SET sentiment_score = ? WHERE tweet_id = ?\"\n","        cursor.executemany(update_query, batch)\n","        connection.commit()\n","    except sqlite3.Error as e:\n","        print(f\"Error updating batch: {e}\")\n","    finally:\n","        if cursor:\n","            cursor.close()\n","        if connection:\n","            connection.close()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T14:10:40.005346Z","iopub.status.busy":"2024-05-31T14:10:40.004708Z","iopub.status.idle":"2024-05-31T14:11:16.320086Z","shell.execute_reply":"2024-05-31T14:11:16.319147Z","shell.execute_reply.started":"2024-05-31T14:10:40.005314Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Start the upload\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa04a6f2f2704af18144e8e7b55c01d3","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f438000b1524b5f8b8a87259296d734","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48ff80221eba497f98385685f7416d33","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13e3ccdc83cd47a1968f513dad8eb213","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21c4e6e262a24b4fa547af81af5c47f0","version_major":2,"version_minor":0},"text/plain":["tf_model.h5:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Physical devices cannot be modified after being initialized\n"]}],"source":["print(\"Start the upload\")\n","path = os.path.join(\"data_processed\", \"local_backup.db\")\n","query = \"\"\"\n","SELECT tweet_id, full_text\n","FROM Tweets\n","WHERE sentiment_score IS NULL;\n","\"\"\"\n","batch_size = 5_000\n","# Load the tokenizer and model\n","\n","model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n","if physical_devices := tf.config.list_physical_devices('GPU'):\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","    except RuntimeError as e:\n","        print(e)\n","\n","device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T15:26:34.295222Z","iopub.status.busy":"2024-05-31T15:26:34.294824Z","iopub.status.idle":"2024-05-31T15:26:38.508638Z","shell.execute_reply":"2024-05-31T15:26:38.507660Z","shell.execute_reply.started":"2024-05-31T15:26:34.295189Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T14:11:31.155149Z","iopub.status.busy":"2024-05-31T14:11:31.154747Z","iopub.status.idle":"2024-05-31T14:11:42.470040Z","shell.execute_reply":"2024-05-31T14:11:42.469028Z","shell.execute_reply.started":"2024-05-31T14:11:31.155096Z"},"trusted":true},"outputs":[],"source":["# path = \"/kaggle/working/local_backup.db\"\n","path =  os.path.join(os.path.dirname(os.getcwd()),\n","                     \"data_processed\",\n","                     \"local_backup.db\")\n","test_data = fetch_data(query, path)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T14:11:42.471480Z","iopub.status.busy":"2024-05-31T14:11:42.471190Z","iopub.status.idle":"2024-05-31T14:12:03.993333Z","shell.execute_reply":"2024-05-31T14:12:03.992285Z","shell.execute_reply.started":"2024-05-31T14:11:42.471456Z"},"trusted":true},"outputs":[],"source":["# Airlines list\n","airlines = [\n","    'KLM', \"British_Airways\", \"airfrance\", \"AmericanAir\", \"lufthansa\", \n","    \"airberlinAssist\", \"easyJet\", \"Ryanair\", \"SingaporeAir\", \"Qantas\",\n","    \"EtihadAirways\", \"VirginAtlantic\", \"airberlin\"\n","]\n","\n","# Function to clean mentions not in the airlines list\n","def clean_mentions(text):\n","    # Regex pattern to find mentions\n","    mention_pattern = r'@([A-Za-z0-9_]+)'\n","    mentions = re.findall(mention_pattern, text)\n","    \n","    # Check if each mention is in the airlines list\n","    valid_mentions = [f\"@{mention}\" for mention in mentions if mention in airlines]\n","    \n","    # Replace invalid mentions in the text\n","    for mention in mentions:\n","        if mention not in airlines:\n","            text = text.replace(f\"@{mention}\", \"\")\n","    \n","    return text\n","\n","# Apply the cleaning function to the DataFrame\n","test_data['cleaned_text'] = test_data['full_text'].apply(clean_mentions)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T14:12:03.994984Z","iopub.status.busy":"2024-05-31T14:12:03.994581Z","iopub.status.idle":"2024-05-31T14:12:04.015328Z","shell.execute_reply":"2024-05-31T14:12:04.014177Z","shell.execute_reply.started":"2024-05-31T14:12:03.994948Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>full_text</th>\n","      <th>cleaned_text</th>\n","    </tr>\n","    <tr>\n","      <th>tweet_id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1151900551367614465</th>\n","      <td>\"In 2019, I just find it the weirdest thing go...</td>\n","      <td>\"In 2019, I just find it the weirdest thing go...</td>\n","    </tr>\n","    <tr>\n","      <th>1151902427676925952</th>\n","      <td>Delighted to see @IrishTimes contributors slow...</td>\n","      <td>Delighted to see  contributors slowly but sure...</td>\n","    </tr>\n","    <tr>\n","      <th>1151902431015518208</th>\n","      <td>@KLM @HeatherYemm Why don’t the passengers jus...</td>\n","      <td>@KLM  Why don’t the passengers just cover thei...</td>\n","    </tr>\n","    <tr>\n","      <th>1151902438988947456</th>\n","      <td>RT @clara_wichmann: Please ⁦@KLM⁩ mogen kleine...</td>\n","      <td>RT : Please ⁦@KLM⁩ mogen kleine babytjes ook w...</td>\n","    </tr>\n","    <tr>\n","      <th>1151902440360484865</th>\n","      <td>@KLM @HeatherYemm WTF?? Permitted??? Det er (e...</td>\n","      <td>@KLM  WTF?? Permitted??? Det er (eneste) mad t...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1244696703690772485</th>\n","      <td>RT @jfergo86: Me parece a mí o el avión es más...</td>\n","      <td>RT : Me parece a mí o el avión es más grande q...</td>\n","    </tr>\n","    <tr>\n","      <th>1244696708983984131</th>\n","      <td>Today’s random pic of the day is the one of Vo...</td>\n","      <td>Today’s random pic of the day is the one of Vo...</td>\n","    </tr>\n","    <tr>\n","      <th>1244696710447800320</th>\n","      <td>RT @SchipholWatch: @spbverhagen @markduursma @...</td>\n","      <td>RT :    @KLM   Nog niet aan de orde? Als in: e...</td>\n","    </tr>\n","    <tr>\n","      <th>1244696713350217728</th>\n","      <td>RT @wiltingklaas: Tweede Kamer stemt over vlie...</td>\n","      <td>RT : Tweede Kamer stemt over vliegtaks https:/...</td>\n","    </tr>\n","    <tr>\n","      <th>1244696713765564416</th>\n","      <td>@easyJet My refund is being process since two ...</td>\n","      <td>@easyJet My refund is being process since two ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4918105 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                             full_text  \\\n","tweet_id                                                                 \n","1151900551367614465  \"In 2019, I just find it the weirdest thing go...   \n","1151902427676925952  Delighted to see @IrishTimes contributors slow...   \n","1151902431015518208  @KLM @HeatherYemm Why don’t the passengers jus...   \n","1151902438988947456  RT @clara_wichmann: Please ⁦@KLM⁩ mogen kleine...   \n","1151902440360484865  @KLM @HeatherYemm WTF?? Permitted??? Det er (e...   \n","...                                                                ...   \n","1244696703690772485  RT @jfergo86: Me parece a mí o el avión es más...   \n","1244696708983984131  Today’s random pic of the day is the one of Vo...   \n","1244696710447800320  RT @SchipholWatch: @spbverhagen @markduursma @...   \n","1244696713350217728  RT @wiltingklaas: Tweede Kamer stemt over vlie...   \n","1244696713765564416  @easyJet My refund is being process since two ...   \n","\n","                                                          cleaned_text  \n","tweet_id                                                                \n","1151900551367614465  \"In 2019, I just find it the weirdest thing go...  \n","1151902427676925952  Delighted to see  contributors slowly but sure...  \n","1151902431015518208  @KLM  Why don’t the passengers just cover thei...  \n","1151902438988947456  RT : Please ⁦@KLM⁩ mogen kleine babytjes ook w...  \n","1151902440360484865  @KLM  WTF?? Permitted??? Det er (eneste) mad t...  \n","...                                                                ...  \n","1244696703690772485  RT : Me parece a mí o el avión es más grande q...  \n","1244696708983984131  Today’s random pic of the day is the one of Vo...  \n","1244696710447800320  RT :    @KLM   Nog niet aan de orde? Als in: e...  \n","1244696713350217728  RT : Tweede Kamer stemt over vliegtaks https:/...  \n","1244696713765564416  @easyJet My refund is being process since two ...  \n","\n","[4918105 rows x 2 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["test_data"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T14:30:23.775166Z","iopub.status.busy":"2024-05-31T14:30:23.774461Z","iopub.status.idle":"2024-05-31T14:30:24.018583Z","shell.execute_reply":"2024-05-31T14:30:24.017776Z","shell.execute_reply.started":"2024-05-31T14:30:23.775134Z"},"trusted":true},"outputs":[],"source":["data_batches = get_batches(test_data[[\"cleaned_text\"]], batch_size)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T14:30:25.731004Z","iopub.status.busy":"2024-05-31T14:30:25.730204Z","iopub.status.idle":"2024-05-31T15:10:35.183327Z","shell.execute_reply":"2024-05-31T15:10:35.182130Z","shell.execute_reply.started":"2024-05-31T14:30:25.730974Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Updating text:   2%|▏         | 17/984 [38:45<36:52:25, 137.28s/it]Exception ignored in: <function _xla_gc_callback at 0x7cd54e355f30>\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.10/site-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","KeyboardInterrupt: \n","Exception ignored in: <function _xla_gc_callback at 0x7cd54e355f30>\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.10/site-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","KeyboardInterrupt: \n","Exception ignored in: <function _xla_gc_callback at 0x7cd54e355f30>\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.10/site-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","KeyboardInterrupt: \n","Exception ignored in: <function _xla_gc_callback at 0x7cd54e355f30>\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.10/site-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","KeyboardInterrupt: \n","Updating text:   2%|▏         | 17/984 [40:09<38:04:09, 141.73s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(data_batches, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating text: \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     df_sentiment \u001b[38;5;241m=\u001b[39m \u001b[43mapply_sentiment_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     update_text_local(convert_to_list(df_sentiment), path)\n","Cell \u001b[0;32mIn[12], line 60\u001b[0m, in \u001b[0;36mapply_sentiment_analysis\u001b[0;34m(df, text_column, batch_size, max_workers)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size):\n\u001b[1;32m     59\u001b[0m         batch \u001b[38;5;241m=\u001b[39m texts[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 60\u001b[0m         results\u001b[38;5;241m.\u001b[39mextend(\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     61\u001b[0m         clear_gpu_memory()\n\u001b[1;32m     62\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m results\n","File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n","File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for batch in tqdm(data_batches, desc=\"Updating text: \"):\n","    df_sentiment = apply_sentiment_analysis(batch, \"cleaned_text\", 32, 2)\n","    update_text_local(convert_to_list(df_sentiment), path)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5124736,"sourceId":8570762,"sourceType":"datasetVersion"},{"datasetId":5125348,"sourceId":8571611,"sourceType":"datasetVersion"}],"dockerImageVersionId":30716,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
