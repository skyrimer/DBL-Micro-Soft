{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN BECAUSE IT IS FUCKED UP\n",
    "\n",
    "\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.special import softmax\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_data(query: str, path: str) -> pd.DataFrame:\n",
    "    with sqlite3.connect(path) as connection:\n",
    "        return pd.read_sql_query(query, connection, index_col='tweet_id')\n",
    "\n",
    "\n",
    "def process_batch(texts):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to a batch of texts using a pre-trained transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list): A list of texts to analyze.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of ranked sentiment labels for the input texts.\n",
    "\n",
    "    This function uses a pre-trained transformer model for sequence classification to analyze the sentiment of the texts in the given batch. It returns a list of ranked sentiment labels for the input texts, where the first label is the most likely sentiment and the subsequent labels are less likely sentiments.\n",
    "    \"\"\"\n",
    "    # Get the maximum sequence length for the model\n",
    "\n",
    "    encoded_input = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "    with tf.device(device):\n",
    "        output = model(encoded_input)\n",
    "\n",
    "    scores = output[0].numpy()\n",
    "    scores = softmax(scores, axis=1)\n",
    "\n",
    "    sentiment_scores = scores[:, 2] - scores[:, 0]\n",
    "    return sentiment_scores.tolist()\n",
    "\n",
    "# Clear GPU memory function\n",
    "def clear_gpu_memory():\n",
    "    tf.keras.backend.clear_session()  # Clear the current session\n",
    "    try:\n",
    "        tf.compat.v1.reset_default_graph()  # For TensorFlow 1.x compatibility\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    gc.collect()  # Explicitly call the garbage collector\n",
    "\n",
    "\n",
    "def apply_sentiment_analysis(df, text_column, batch_size=128, max_workers=4):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to the given DataFrame using a pre-trained transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the text column to analyze.\n",
    "    text_column (str): The name of the column in the DataFrame containing the text to analyze.\n",
    "    batch_size (int): The number of texts to process in each batch. Default is 128.\n",
    "    max_workers (int): The maximum number of worker threads to use for parallel processing. Default is 4.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The input DataFrame with an additional 'sentiment' column containing the sentiment analysis results.\n",
    "\n",
    "    This function uses a pre-trained transformer model for sequence classification to analyze the sentiment of the texts in the given DataFrame. It applies the sentiment analysis in parallel using multiple worker threads to improve performance. The results are then added to the input DataFrame as a new 'sentiment' column.\n",
    "    \"\"\"\n",
    "    texts = df[text_column].tolist()\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            results.extend(executor.submit(process_batch, batch).result())\n",
    "            clear_gpu_memory()\n",
    "\n",
    "    df[\"sentiment\"] = results\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_batches(df: pd.DataFrame, batch_size: int = 1000) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split DataFrame into batches of DataFrames with specified batch size.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame containing tweet data.\n",
    "        batch_size: The size of each batch.\n",
    "\n",
    "    Returns:\n",
    "        A list of DataFrames, each containing a batch of rows.\n",
    "    \"\"\"\n",
    "    return [df.iloc[i : i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "def convert_to_list(df: pd.DataFrame) -> List[List]:\n",
    "    \"\"\"\n",
    "    Convert DataFrame with tweet_id as index to a list of lists containing sentiment and tweet_id.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame with tweet_id as index and sentiment as a column.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists containing tweet_id and sentiment.\n",
    "    \"\"\"\n",
    "    tweet_ids = df.index.to_numpy()\n",
    "    sentiments = df[\"sentiment\"].to_numpy()\n",
    "    return tuple(zip(sentiments, tweet_ids))\n",
    "\n",
    "def connect_to_local_database(db_path: str):\n",
    "    \"\"\"\n",
    "    Establish a connection to the local SQLite database.\n",
    "\n",
    "    Args:\n",
    "        db_path: The path to the SQLite database file.\n",
    "\n",
    "    Returns:\n",
    "        A connection object to the SQLite database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return sqlite3.connect(db_path)\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error while connecting to SQLite: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def update_text_local(\n",
    "    batch: List[Tuple[str, str]], db_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Update full_text values for a batch of data in the local SQLite database.\n",
    "\n",
    "    Args:\n",
    "        batch: List of (full_text, tweet_id) pairs.\n",
    "        db_path: The path to the SQLite database file.\n",
    "    \"\"\"\n",
    "    connection = connect_to_local_database(db_path)\n",
    "    if connection is None:\n",
    "        return\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        update_query = \"UPDATE Tweets SET sentiment_score = ? WHERE tweet_id = ?\"\n",
    "        cursor.executemany(update_query, batch)\n",
    "        connection.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error updating batch: {e}\")\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection:\n",
    "            connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the upload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\My programs\\Anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chekm\\AppData\\Local\\Temp\\ipykernel_16840\\1770327625.py:16: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "print(\"Start the upload\")\n",
    "path = os.path.join(\n",
    "        os.path.dirname(\n",
    "            os.getcwd()\n",
    "        ),\n",
    "    \"data_processed\", \"local_backup.db\")\n",
    "query = \"\"\"\n",
    "SELECT tweet_id, full_text\n",
    "FROM Tweets\n",
    "WHERE sentiment_score IS NULL;\n",
    "\"\"\"\n",
    "batch_size = 5_000\n",
    "# Load the tokenizer and model\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    ")\n",
    "# Check if GPU is available\n",
    "device = \"/GPU:0\" if tf.test.is_gpu_available() else \"/CPU:0\"\n",
    "if gpus := tf.config.experimental.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = fetch_data(query, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airlines list\n",
    "airlines = [\n",
    "    'KLM', \"British_Airways\", \"airfrance\", \"AmericanAir\", \"lufthansa\", \n",
    "    \"airberlinAssist\", \"easyJet\", \"Ryanair\", \"SingaporeAir\", \"Qantas\",\n",
    "    \"EtihadAirways\", \"VirginAtlantic\", \"airberlin\"\n",
    "]\n",
    "\n",
    "# Function to clean mentions not in the airlines list\n",
    "def clean_mentions(text):\n",
    "    # Regex pattern to find mentions\n",
    "    mention_pattern = r'@([A-Za-z0-9_]+)'\n",
    "    mentions = re.findall(mention_pattern, text)\n",
    "    \n",
    "    # Check if each mention is in the airlines list\n",
    "    valid_mentions = [f\"@{mention}\" for mention in mentions if mention in airlines]\n",
    "    \n",
    "    # Replace invalid mentions in the text\n",
    "    for mention in mentions:\n",
    "        if mention not in airlines:\n",
    "            text = text.replace(f\"@{mention}\", \"\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the DataFrame\n",
    "test_data['cleaned_text'] = test_data['full_text'].apply(clean_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1137380899622858752</th>\n",
       "      <td>Prove it. Receipts please. (Not saying I don't...</td>\n",
       "      <td>Prove it. Receipts please. (Not saying I don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137380903548792832</th>\n",
       "      <td>RT @AdaraCensored: .@Delta @AmericanAir @JetBl...</td>\n",
       "      <td>RT : . @AmericanAir   \\nI am very concerned th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137380905792745473</th>\n",
       "      <td>RT @kjbelfon: I think we (especially gay men) ...</td>\n",
       "      <td>RT : I think we (especially gay men) need to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137380911178276864</th>\n",
       "      <td>RT @emcjayyy: Next time you say “sO wHeN iS sT...</td>\n",
       "      <td>RT : Next time you say “sO wHeN iS sTrAiGhT pR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137380913837420544</th>\n",
       "      <td>RT @GovMikeHuckabee: In CO Springs and thanks ...</td>\n",
       "      <td>RT : In CO Springs and thanks to @AmericanAir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696703690772485</th>\n",
       "      <td>RT @jfergo86: Me parece a mí o el avión es más...</td>\n",
       "      <td>RT : Me parece a mí o el avión es más grande q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696708983984131</th>\n",
       "      <td>Today’s random pic of the day is the one of Vo...</td>\n",
       "      <td>Today’s random pic of the day is the one of Vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696710447800320</th>\n",
       "      <td>RT @SchipholWatch: @spbverhagen @markduursma @...</td>\n",
       "      <td>RT :    @KLM   Nog niet aan de orde? Als in: e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696713350217728</th>\n",
       "      <td>RT @wiltingklaas: Tweede Kamer stemt over vlie...</td>\n",
       "      <td>RT : Tweede Kamer stemt over vliegtaks https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244696713765564416</th>\n",
       "      <td>@easyJet My refund is being process since two ...</td>\n",
       "      <td>@easyJet My refund is being process since two ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5788105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             full_text  \\\n",
       "tweet_id                                                                 \n",
       "1137380899622858752  Prove it. Receipts please. (Not saying I don't...   \n",
       "1137380903548792832  RT @AdaraCensored: .@Delta @AmericanAir @JetBl...   \n",
       "1137380905792745473  RT @kjbelfon: I think we (especially gay men) ...   \n",
       "1137380911178276864  RT @emcjayyy: Next time you say “sO wHeN iS sT...   \n",
       "1137380913837420544  RT @GovMikeHuckabee: In CO Springs and thanks ...   \n",
       "...                                                                ...   \n",
       "1244696703690772485  RT @jfergo86: Me parece a mí o el avión es más...   \n",
       "1244696708983984131  Today’s random pic of the day is the one of Vo...   \n",
       "1244696710447800320  RT @SchipholWatch: @spbverhagen @markduursma @...   \n",
       "1244696713350217728  RT @wiltingklaas: Tweede Kamer stemt over vlie...   \n",
       "1244696713765564416  @easyJet My refund is being process since two ...   \n",
       "\n",
       "                                                          cleaned_text  \n",
       "tweet_id                                                                \n",
       "1137380899622858752  Prove it. Receipts please. (Not saying I don't...  \n",
       "1137380903548792832  RT : . @AmericanAir   \\nI am very concerned th...  \n",
       "1137380905792745473  RT : I think we (especially gay men) need to d...  \n",
       "1137380911178276864  RT : Next time you say “sO wHeN iS sTrAiGhT pR...  \n",
       "1137380913837420544  RT : In CO Springs and thanks to @AmericanAir ...  \n",
       "...                                                                ...  \n",
       "1244696703690772485  RT : Me parece a mí o el avión es más grande q...  \n",
       "1244696708983984131  Today’s random pic of the day is the one of Vo...  \n",
       "1244696710447800320  RT :    @KLM   Nog niet aan de orde? Als in: e...  \n",
       "1244696713350217728  RT : Tweede Kamer stemt over vliegtaks https:/...  \n",
       "1244696713765564416  @easyJet My refund is being process since two ...  \n",
       "\n",
       "[5788105 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batches = get_batches(test_data[[\"cleaned_text\"]], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_to_list(data_batches[0])\n",
    "#sentiment_with_score = apply_sentiment_analysis(data_batches[0], \"cleaned_text\", 16, 14)\n",
    "#sentiment_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating text:   0%|                                                                          | 0/1158 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Updating text:   0%|▏                                                            | 3/1158 [18:25<118:16:47, 368.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(data_batches, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating text: \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     df_sentiment \u001b[38;5;241m=\u001b[39m \u001b[43mapply_sentiment_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     update_text_local(convert_to_list(df_sentiment), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m      4\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\n\u001b[0;32m      5\u001b[0m             os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m      6\u001b[0m         ),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_processed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_backup.db\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[2], line 61\u001b[0m, in \u001b[0;36mapply_sentiment_analysis\u001b[1;34m(df, text_column, batch_size, max_workers)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size):\n\u001b[0;32m     60\u001b[0m         batch \u001b[38;5;241m=\u001b[39m texts[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m---> 61\u001b[0m         results\u001b[38;5;241m.\u001b[39mextend(\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m         clear_gpu_memory()\n\u001b[0;32m     64\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[1;32mC:\\My programs\\Anaconda\\lib\\concurrent\\futures\\_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mC:\\My programs\\Anaconda\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in tqdm(data_batches, desc=\"Updating text: \"):\n",
    "    df_sentiment = apply_sentiment_analysis(batch, \"cleaned_text\", 8, 4)\n",
    "    update_text_local(convert_to_list(df_sentiment), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
